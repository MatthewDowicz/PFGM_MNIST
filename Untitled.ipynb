{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9518038b-6f13-430d-bd61-3acff13c49d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdowicz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing fonts to be latex typesetting\n",
    "from matplotlib import rcParams\n",
    "rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "rcParams['font.family'] = 'serif'\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "# jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "from flax.serialization import (\n",
    "    to_state_dict, msgpack_serialize, from_bytes\n",
    ")\n",
    "import optax\n",
    "\n",
    "# For ODESolver\n",
    "from scipy import integrate\n",
    "\n",
    "# PyTorch for Dataloaders\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Wandb \n",
    "import wandb\n",
    "wandb.login()\n",
    "import pprint\n",
    "\n",
    "# Import created functions\n",
    "import make_dataset as mkds\n",
    "import visualization as vis\n",
    "# import NN_model as nnm\n",
    "# import observable_data as od\n",
    "\n",
    "\n",
    "from numpy.random import default_rng\n",
    "prng = jax.random.PRNGKey(42)\n",
    "rng = default_rng(seed=np.asarray(prng))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c361fe9-d40d-44dc-9730-01b59ef41eb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8738285-0115-4187-ab9e-a4950fd9f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP model for testing PFGM.\n",
    "\n",
    "    Due to it's simplicity we use @nn.compact instead of setup\n",
    "    \"\"\"\n",
    "    hidden_dims: Sequence[int]\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, **kwargs):\n",
    "        for dim in self.hidden_dims:\n",
    "            x = nn.Dense(dim)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "    \n",
    "def init_train_state(model: Any,\n",
    "                     random_key: Any,\n",
    "                     shape: tuple,\n",
    "                     learning_rate: int) -> train_state.TrainState:\n",
    "    \"\"\"\n",
    "    Function to initialize the TrainState dataclass, which represents\n",
    "    the entire training state, including step number, parameters, and \n",
    "    optimizer state. This is useful because we no longer need to\n",
    "    initialize the model again and again with new variables, we just \n",
    "    update the \"state\" of the mdoel and pass this as inputs to functions.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        model: nn.Module    \n",
    "            The model that we want to train.\n",
    "        random_key: jax.random.PRNGKey()\n",
    "            Used to trigger the initialization functions, which generate\n",
    "            the initial set of parameters that the model will use.\n",
    "        shape: tuple\n",
    "            Shape of the batch of data that will be input into the model.\n",
    "            This is used to trigger shape inference, which is where the model\n",
    "            figures out by itself what the correct size the weights should be\n",
    "            when they see the inputs.\n",
    "        learning_rate: int\n",
    "            How large of a step the optimizer should take.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        train_state.TrainState:\n",
    "            A utility class for handling parameter and gradient updates. \n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    variables = model.init(random_key, jnp.ones(shape))\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate) # TODO update this to be user defined\n",
    "\n",
    "    # Create a state\n",
    "    return train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                         tx=optimizer,\n",
    "                                         params=variables['params'])\n",
    "\n",
    "def compute_metrics(*, pred, labels):\n",
    "    \"\"\"\n",
    "    Function that computes metrics that will be logged\n",
    "    during training\n",
    "    \"\"\"\n",
    "    # Calculate the MSE loss\n",
    "    loss = ((pred - labels) ** 2).mean()\n",
    "\n",
    "    # Calculate the R^2 score\n",
    "    residual = jnp.sum(jnp.square(labels - pred))\n",
    "    total = jnp.sum(jnp.square(labels - jnp.mean(labels)))\n",
    "    r2_score = 1 - (residual / total)\n",
    "\n",
    "    # Save these metrics into a dict\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'r2': r2_score\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def accumulate_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Function that accumulates all the metrics for each batch and \n",
    "    accumulates/calculates the metrics for each epoch.\n",
    "    \"\"\"\n",
    "    metrics = jax.device_get(metrics)\n",
    "    return {\n",
    "        k: np.mean([metric[k] for metric in metrics])\n",
    "        for k in metrics[0]\n",
    "    }\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: train_state.TrainState,\n",
    "               batch: list):\n",
    "    \"\"\"\n",
    "    Function to run training on one batch of data.\n",
    "    \"\"\"\n",
    "    image, label = batch\n",
    "\n",
    "    def loss_fn(params: dict):\n",
    "        \"\"\"\n",
    "        Simple MSE loss as described in the PFGM paper.\n",
    "        \"\"\"\n",
    "        pred = state.apply_fn({'params': params}, image)\n",
    "        loss = ((pred - label) ** 2).mean()\n",
    "        return loss, pred\n",
    "\n",
    "    def r_squared(params):\n",
    "        \"\"\"\n",
    "        Function to calculate the coefficient of determination or \n",
    "        R^2, which quantifies how well the regression model fits \n",
    "        the observed data. Or more formally, it is a statistical\n",
    "        measure that represents the proportion of variance in the\n",
    "        dependent variable that is explained by the independent \n",
    "        variable(s) in a regression model. R^2 ranges from 0 to 1, \n",
    "        with a higher value indicating a better fit. \n",
    "\n",
    "        An R^2 of 0 means that the regression model does not explain\n",
    "        any of the variability in the dependent variable, while an\n",
    "        R^2 of 1 indicates that the regression model explains all of\n",
    "        the variability in the dependent model.\n",
    "        \"\"\"\n",
    "        pred = state.apply_fn({'params': params}, image)\n",
    "        residual = jnp.sum(jnp.square(label - pred))\n",
    "        total = jnp.sum(jnp.square(label - jnp.mean(label)))\n",
    "        r2_score = 1 - (residual / total)\n",
    "        return r2_score\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, pred), grads = gradient_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(pred=pred, labels=label)\n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    image, label = batch\n",
    "    pred = state.apply_fn({'params': state.params}, image)\n",
    "    return compute_metrics(pred=pred, labels=label)\n",
    "\n",
    "\n",
    "def save_checkpoint_wandb(ckpt_path, state, epoch):\n",
    "    with open(ckpt_path, \"wb\") as outfile:\n",
    "        outfile.write(msgpack_serialize(to_state_dict(state)))\n",
    "    artifact = wandb.Artifact(\n",
    "        f'{wandb.run.name}-checkpoint', type='dataset'\n",
    "    )\n",
    "    artifact.add_file(ckpt_path)\n",
    "    wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "    \n",
    "def load_checkpoint_wandb(ckpt_file, state):\n",
    "    artifact = wandb.use_artifact(\n",
    "        f'{wandb.run.name}-checkpoint:latest'\n",
    "    )\n",
    "    artifact_dir = artifact.download()\n",
    "    ckpt_path = os.path.join(artifact_dir, ckpt_file)\n",
    "    with open(ckpt_path, \"rb\") as data_file:\n",
    "        byte_data = data_file.read()\n",
    "    return from_bytes(state, byte_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477cfa5-e07c-460f-9516-51a5062472a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Creating perturbed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8881d9-2103-4170-a663-b12e808d9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_DS(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset class that stores the data and targets as NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        data: np.ndarray\n",
    "            The perturbed input data.\n",
    "        targets: np.ndarray\n",
    "            The empirical field that generated the perturbed data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the i-th sample and corresponding target in the dataset.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            idx: int\n",
    "                The index of the sample to return.\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple: A tuple containing the sample and target.\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "    \n",
    "    \n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def create_data_loaders(*datasets : Sequence[data.Dataset],\n",
    "                        train : Union[bool, Sequence[bool]] = True,\n",
    "                        batch_size : int = 128,\n",
    "                        num_workers : int = 4,\n",
    "                        seed : int = 42):\n",
    "    \"\"\"\n",
    "    Creates data loaders used in JAX for a set of datasets.\n",
    "\n",
    "    Args:\n",
    "      datasets: Datasets for which data loaders are created.\n",
    "      train: Sequence indicating which datasets are used for\n",
    "        training and which not. If single bool, the same value\n",
    "        is used for all datasets.\n",
    "      batch_size: Batch size to use in the data loaders.\n",
    "      num_workers: Number of workers for each dataset.\n",
    "      seed: Seed to initialize the workers and shuffling with.\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "    if not isinstance(train, (list, tuple)):\n",
    "        train = [train for _ in datasets]\n",
    "    for dataset, is_train in zip(datasets, train):\n",
    "        loader = torch.utils.data.DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=is_train,\n",
    "                                 drop_last=is_train,\n",
    "                                 collate_fn=numpy_collate,\n",
    "                                 num_workers=num_workers,\n",
    "                                 persistent_workers=is_train,\n",
    "                                 generator=torch.Generator().manual_seed(seed))\n",
    "        loaders.append(loader)\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19716d78-e90b-4388-92ee-f02420215f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = mkds.download_MNIST(download=False)\n",
    "train_data = mkds.reshape_with_channel_dim(train.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a6e8e-f6ac-4189-9f69-9f44997df024",
   "metadata": {},
   "source": [
    "# 2. Creating the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d16d0cd-4e56-4728-82fc-2aa24156a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_perturbed_data(dataset: np.ndarray, \n",
    "                           prng: jax.random.PRNGKey):\n",
    "    \"\"\"\n",
    "    Function that perturbs the raw MNIST files (ie. train data file/test data file)\n",
    "    and saves them into a tuple of (perturbed_data, empirical_field).\n",
    "    \n",
    "    It does this by calculating the number of passes needed to go through the entire\n",
    "    dataset if we used a batchsize of 1000. The reason for the seemingly arbitrary choice\n",
    "    of 1000, is that I'm running out of memory at when my batchsize is larger than 1000.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        dataset: np.ndarray\n",
    "            Either the raw MNIST training/testing dataset.\n",
    "            NOTE: Dataset here refers to the pixel data only, we discard the labels because\n",
    "                  in PFGM the NN learns the empirical field at each pixel location. One can\n",
    "                  think of the empirical field at each pixel location as that pixels \"label\".\n",
    "                  \n",
    "        prng: jax.random.PRNGKey\n",
    "            Source of randomness for generating random numbers  \n",
    "            \n",
    "    Returns:\n",
    "    --------\n",
    "        tuple of the perturbed data for each sample and the empirical field for each image\n",
    "    \"\"\"\n",
    "    # Get the number of samples in the dataset\n",
    "    num_samples = dataset.shape[0]\n",
    "    # Calculate the number of passes needed to go through the entire dataset\n",
    "    # in batches of 1000\n",
    "    num_passes = int(np.ceil(num_samples / 1000))\n",
    "    # Initialize empty list to store the output of the function\n",
    "    outputs = []\n",
    "    # Loop over the number passes needed to go through entire dataset\n",
    "    for i in range(num_passes):\n",
    "        # Calculate start and end indices for the current batch\n",
    "        start_idx = i * 1000\n",
    "        end_idx = min((i+1)*1000, num_samples)\n",
    "        # Get the current batch from the dataset\n",
    "        batch = dataset[start_idx:end_idx]\n",
    "        # Get the perturbed data & empirical field for that batch\n",
    "        output = mkds.empirical_field(batch, prng)\n",
    "        outputs.append(output)\n",
    "        del output\n",
    "    # Concatenate outputs along the first axis\n",
    "    return tuple(np.concatenate(o, axis=0) for o in zip(*outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352067b9-af12-4607-badb-fbeef14f943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbed_data = process_perturbed_data(train_data, prng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae43ad5-6a9d-45d7-b638-ed2f093f90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_MNIST(root_dir: str = 'saved_data/MNIST/perturbed/partitioned/',\n",
    "                    perturb_on: bool = True,\n",
    "                    sigma: float = 0.2, \n",
    "                    tau: float = 0.06, \n",
    "                    M: int = 291,\n",
    "                    download: bool = True,\n",
    "                    validation_frac: float = 1/6):\n",
    "    \"\"\"\n",
    "    Function to partition the raw/perturbed training/test data into training, validation, and test\n",
    "    datasets. The split will be 50k, 10k, and 10k, where the validation set will be a random\n",
    "    sampling without replacement from the raw training set.\n",
    "    \n",
    "    Args:\n",
    "    ------\n",
    "        root_dir: str\n",
    "            Path to where the partitioned data should saved to.\n",
    "        perturb_on: bool\n",
    "            If True, partition the perturbed data consisting of the perturbed data & the empirical\n",
    "            field. Else, partition the raw unperturbed MNIST data.\n",
    "        download: bool\n",
    "             If True, download the partition dataset to the root_dir. Else, simply return the partitioned\n",
    "             dataset.\n",
    "     \n",
    "     Returns:\n",
    "     --------\n",
    "         training_set: np.ndarray\n",
    "             The 50k perturbed samples that will be used for training the NN.\n",
    "         val_set: np.ndarray\n",
    "             The 10K perturbed samples that will be used for validating the NN. These 10k samples came\n",
    "             from the raw MNIST training set. \n",
    "         test_set: np.ndarray\n",
    "             The 10k samples that will be used to test the NN. This is the perturbed version of the raw \n",
    "             MNIST test set.\n",
    "    \"\"\"\n",
    "    # Instantiate the MNIST data\n",
    "    train, test = mkds.download_MNIST(download=False)\n",
    "    \n",
    "    # Give a channel dimension to the data\n",
    "    train_data = mkds.reshape_with_channel_dim(train.data.numpy())\n",
    "    test_data = mkds.reshape_with_channel_dim(test.data.numpy())\n",
    "    \n",
    "    # Perturb the data\n",
    "    perturbed_train = process_perturbed_data(train_data, prng=rng)\n",
    "    perturbed_test = process_perturbed_data(test_data, prng=rng)\n",
    "    \n",
    "    # Define the fraction of data to use for validation\n",
    "    validation_frac = validation_frac\n",
    "\n",
    "    # Get the number of samples in the training set\n",
    "    num_samples = perturbed_train[0].shape[0]\n",
    "\n",
    "    # Generate a random permutatioon of the sample indices\n",
    "    permutation = np.random.permutation(num_samples)\n",
    "\n",
    "    # Calculate the number of samples to use for validation\n",
    "    num_validation_samples = int(num_samples * validation_frac)\n",
    "\n",
    "    # Split the permutation into training and validation indices\n",
    "    validation_indices = permutation[:num_validation_samples]\n",
    "    training_indices = permutation[num_validation_samples:]\n",
    "\n",
    "    # Split the training set into training and validation sets\n",
    "    X_train_new = perturbed_train[0][training_indices]\n",
    "    y_train_new = perturbed_train[1][training_indices]\n",
    "    train_new = (X_train_new, y_train_new)\n",
    "\n",
    "    X_val = perturbed_train[0][validation_indices]\n",
    "    y_val = perturbed_train[1][validation_indices]\n",
    "    val = (X_val, y_val)\n",
    "    \n",
    "    if download:\n",
    "        # Save the data\n",
    "        mkds.save_data(train_new, \n",
    "                   directory=root_dir,\n",
    "                   filename='perturbed_training_set.pkl')\n",
    "\n",
    "        mkds.save_data(val, \n",
    "                   directory=root_dir,\n",
    "                   filename='perturbed_val_set.pkl')\n",
    "\n",
    "        mkds.save_data(perturbed_test, \n",
    "                   directory=root_dir,\n",
    "                   filename='perturbed_test_set.pkl')\n",
    "    \n",
    "    return train_new, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea9948b-9413-4fd5-9ac3-e781affef2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test = partition_MNIST(download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2d315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd620a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "998bf411-c709-4921-9417-a89c20c59152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(batchsize, state, epochs, ckpt_dir, prng):    \n",
    "\n",
    "    # Load the partitioned datasets\n",
    "    training = mkds.load_data(data_dir='saved_data/MNIST/perturbed/partitioned',\n",
    "                                         data_file='perturbed_training_set.pkl')\n",
    "\n",
    "    val = mkds.load_data(data_dir='saved_data/MNIST/perturbed/partitioned',\n",
    "                                         data_file='perturbed_val_set.pkl')\n",
    "\n",
    "    test = mkds.load_data(data_dir='saved_data/MNIST/perturbed/partitioned',\n",
    "                                         data_file='perturbed_test_set.pkl')\n",
    "\n",
    "    print('data loaded')\n",
    "    # Put them into Pytorch Dataset Objects\n",
    "    training = MNIST_DS(training[0], training[1])\n",
    "    val = MNIST_DS(val[0], val[1])\n",
    "    testing = MNIST_DS(test[0], test[1])\n",
    "    \n",
    "    # Instantiate the pytorch data loaders\n",
    "    train_dl = DataLoader(training, batch_size=batchsize, collate_fn=numpy_collate, shuffle=True)\n",
    "    val_dl = DataLoader(val, batch_size=batchsize, collate_fn=numpy_collate, shuffle=False)\n",
    "    test_dl = DataLoader(testing, batch_size=batchsize, collate_fn=numpy_collate, shuffle=False)\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        best_val_loss = 1e6\n",
    "\n",
    "        # =========== Training =========== #\n",
    "        train_batch_metrics = []\n",
    "        for cnt, batch in enumerate(train_dl):\n",
    "            # Instantiate the perturbed data\n",
    "            data, targets = batch\n",
    "            # data = mkds.reshape_with_channel_dim(data)\n",
    "            # perturbed_batch = mkds.empirical_field(data, prng)\n",
    "            # Do one train step with perturbed data\n",
    "            state, metrics = train_step(state, batch)\n",
    "            train_batch_metrics.append(metrics)\n",
    "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
    "        print(\n",
    "            'TRAIN (%d/%d): Loss: %.4f, r2: %.2f' % (\n",
    "                epoch, epochs, train_batch_metrics['loss'], \n",
    "                train_batch_metrics['r2'])\n",
    "        )\n",
    "                \n",
    "        # =========== Validation =========== #\n",
    "        val_batch_metrics = []\n",
    "        for cnt, batch in enumerate(val_dl):\n",
    "            # Instantiate the imgs\n",
    "            data, targets = batch\n",
    "            # data = mkds.reshape_with_channel_dim(data)\n",
    "            # # Perturb the data\n",
    "            # perturbed_batch = mkds.empirical_field(data, prng)\n",
    "            metrics = eval_step(state, batch)\n",
    "            val_batch_metrics.append(metrics)\n",
    "        val_batch_metrics = accumulate_metrics(val_batch_metrics)\n",
    "        print(\n",
    "            'Val (%d/%d): Loss: %.4f, r2: %.2f' % (\n",
    "                epoch, epochs, val_batch_metrics['loss'], \n",
    "                val_batch_metrics['r2'])\n",
    "        )\n",
    "        print()\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_batch_metrics['loss'],\n",
    "            \"Train r2\": train_batch_metrics['r2'],\n",
    "            \"Validation Loss\": val_batch_metrics['loss'],\n",
    "            \"Validation r2\": val_batch_metrics['r2']\n",
    "        }, step=epoch)\n",
    "        \n",
    "        if val_batch_metrics['loss'] < best_val_loss:\n",
    "            save_checkpoint_wandb(\"checkpoint.msgpack\", state, epoch)\n",
    "            \n",
    "    restored_state = load_checkpoint_wandb(\"checkpoint.msgpack\", state)\n",
    "    test_batch_metrics = []\n",
    "    for cnt, batch in enumerate(test_dl):\n",
    "            # Instantiate the imgs\n",
    "            data, targets = batch\n",
    "            # data = mkds.reshape_with_channel_dim(data)\n",
    "            # # Perturb the data\n",
    "            # perturbed_batch = mkds.empirical_field(data, prng)\n",
    "            metrics = eval_step(state, batch)\n",
    "            test_batch_metrics.append(metrics)\n",
    "        \n",
    "    test_batch_metrics = accumulate_metrics(test_batch_metrics)\n",
    "    print(\n",
    "        'Test: Loss: %.4f, r2: %.2f' % (\n",
    "            test_batch_metrics['loss'],\n",
    "            test_batch_metrics['r2']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Test Loss\": test_batch_metrics['loss'],\n",
    "        \"Test r2\": test_batch_metrics['r2']\n",
    "    })\n",
    "\n",
    "    # Save best state\n",
    "    checkpoints.save_checkpoint(ckpt_dir, target=restored_state, step=None)\n",
    "    return state, restored_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5319331",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71a2de5-d102-40bc-ae98-55e4d4e0f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/matt/Desktop/UCI_Research/LSST/PFGM/PFGM_MNIST/wandb/run-20230327_212654-1641t7jh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mdowicz/MLP_PFGM/runs/1641t7jh\" target=\"_blank\">sweet-gorge-118</a></strong> to <a href=\"https://wandb.ai/mdowicz/MLP_PFGM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                  MLP Summary                                   \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams               \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ MLP    │ \u001b[2mfloat32\u001b[0m[1,785]  │ \u001b[2mfloat32\u001b[0m[1,785]  │                       │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,785]  │ \u001b[2mfloat32\u001b[0m[1,1570] │ bias: \u001b[2mfloat32\u001b[0m[1570]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[785,1570]     │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m1,234,020 \u001b[0m\u001b[1;2m(4.9 MB)\u001b[0m    │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,1570] │ \u001b[2mfloat32\u001b[0m[1,3140] │ bias: \u001b[2mfloat32\u001b[0m[3140]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[1570,3140]    │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m4,932,940 \u001b[0m\u001b[1;2m(19.7 MB)\u001b[0m   │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_2 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,3140] │ \u001b[2mfloat32\u001b[0m[1,3140] │ bias: \u001b[2mfloat32\u001b[0m[3140]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[3140,3140]    │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m9,862,740 \u001b[0m\u001b[1;2m(39.5 MB)\u001b[0m   │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_3 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,3140] │ \u001b[2mfloat32\u001b[0m[1,1570] │ bias: \u001b[2mfloat32\u001b[0m[1570]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[3140,1570]    │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m4,931,370 \u001b[0m\u001b[1;2m(19.7 MB)\u001b[0m   │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_4 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,1570] │ \u001b[2mfloat32\u001b[0m[1,785]  │ bias: \u001b[2mfloat32\u001b[0m[785]    │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[1570,785]     │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m1,233,235 \u001b[0m\u001b[1;2m(4.9 MB)\u001b[0m    │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m22,194,305 \u001b[0m\u001b[1;2m(88.8 MB)\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴─────────────────┴─────────────────┴───────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                     Total Parameters: 22,194,305 \u001b[0m\u001b[1;2m(88.8 MB)\u001b[0m\u001b[1m                     \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='MLP_PFGM')\n",
    "\n",
    "config = wandb.config\n",
    "config.jax_seed = 42\n",
    "config.batch_size = 128\n",
    "config.learning_rate = 3e-4\n",
    "config.epochs = 50\n",
    "config.hidden_dims = [1570, 3140, 3140, 1570]\n",
    "config.output_dim = 785\n",
    "\n",
    "# Instantiate the model with random weights\n",
    "mlp = MLP(hidden_dims=config.hidden_dims,\n",
    "          output_dim=config.output_dim)\n",
    "\n",
    "print(mlp.tabulate(jax.random.PRNGKey(config.jax_seed), jnp.ones((1, 785))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5947a6d4-ea4d-4233-8bf7-bc720ffa49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839077c062bd40f784034cec8dc9064c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (1/50): Loss: 2.9721, r2: -1.98\n",
      "Val (1/50): Loss: 0.9986, r2: 0.00\n",
      "\n",
      "TRAIN (2/50): Loss: 0.9985, r2: 0.00\n",
      "Val (2/50): Loss: 0.9985, r2: 0.00\n",
      "\n",
      "TRAIN (3/50): Loss: 0.9984, r2: 0.00\n",
      "Val (3/50): Loss: 0.9983, r2: 0.00\n",
      "\n",
      "TRAIN (4/50): Loss: 0.9982, r2: 0.00\n",
      "Val (4/50): Loss: 0.9982, r2: 0.00\n",
      "\n",
      "TRAIN (5/50): Loss: 0.9981, r2: 0.00\n",
      "Val (5/50): Loss: 0.9981, r2: 0.00\n",
      "\n",
      "TRAIN (6/50): Loss: 0.9981, r2: 0.00\n",
      "Val (6/50): Loss: 0.9981, r2: 0.00\n",
      "\n",
      "TRAIN (7/50): Loss: 0.9980, r2: 0.00\n",
      "Val (7/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (8/50): Loss: 0.9980, r2: 0.00\n",
      "Val (8/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (9/50): Loss: 0.9979, r2: 0.00\n",
      "Val (9/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (10/50): Loss: 0.9979, r2: 0.00\n",
      "Val (10/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (11/50): Loss: 0.9979, r2: 0.00\n",
      "Val (11/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (12/50): Loss: 0.9979, r2: 0.00\n",
      "Val (12/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (13/50): Loss: 0.9979, r2: 0.00\n",
      "Val (13/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (14/50): Loss: 0.9979, r2: 0.00\n",
      "Val (14/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (15/50): Loss: 0.9979, r2: 0.00\n",
      "Val (15/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (16/50): Loss: 0.9979, r2: 0.00\n",
      "Val (16/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (17/50): Loss: 0.9979, r2: 0.00\n",
      "Val (17/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (18/50): Loss: 0.9979, r2: 0.00\n",
      "Val (18/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (19/50): Loss: 0.9979, r2: 0.00\n",
      "Val (19/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (20/50): Loss: 0.9979, r2: 0.00\n",
      "Val (20/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (21/50): Loss: 0.9979, r2: 0.00\n",
      "Val (21/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (22/50): Loss: 0.9979, r2: 0.00\n",
      "Val (22/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (23/50): Loss: 0.9979, r2: 0.00\n",
      "Val (23/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (24/50): Loss: 0.9979, r2: 0.00\n",
      "Val (24/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (25/50): Loss: 0.9979, r2: 0.00\n",
      "Val (25/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (26/50): Loss: 0.9979, r2: 0.00\n",
      "Val (26/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (27/50): Loss: 0.9979, r2: 0.00\n",
      "Val (27/50): Loss: 0.9980, r2: 0.00\n",
      "\n",
      "TRAIN (28/50): Loss: 0.9979, r2: 0.00\n",
      "Val (28/50): Loss: 0.9980, r2: 0.00\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m state \u001b[38;5;241m=\u001b[39m init_train_state(model\u001b[38;5;241m=\u001b[39mmlp, \n\u001b[1;32m      3\u001b[0m                          random_key\u001b[38;5;241m=\u001b[39mjax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(config\u001b[38;5;241m.\u001b[39mjax_seed), \n\u001b[1;32m      4\u001b[0m                          shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m785\u001b[39m), \n\u001b[1;32m      5\u001b[0m                          learning_rate\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m      7\u001b[0m prng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m21\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m state, best_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mckpt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msaved_models/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mprng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprng\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 35\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(batchsize, state, epochs, ckpt_dir, prng)\u001b[0m\n\u001b[1;32m     31\u001b[0m     data, targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# data = mkds.reshape_with_channel_dim(data)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# perturbed_batch = mkds.empirical_field(data, prng)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Do one train step with perturbed data\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     train_batch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     37\u001b[0m train_batch_metrics \u001b[38;5;241m=\u001b[39m accumulate_metrics(train_batch_metrics)\n",
      "File \u001b[0;32m~/.conda/envs/PFGM/lib/python3.9/site-packages/flax/core/frozen_dict.py:159\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, _, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[39m\"\"\"Flattens this FrozenDict.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39m    A flattened version of this FrozenDict instance.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m   \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dict,), ()\n\u001b[0;32m--> 159\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_unflatten\u001b[39m(\u001b[39mcls\u001b[39m, _, data):\n\u001b[1;32m    161\u001b[0m   \u001b[39m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    162\u001b[0m   \u001b[39m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    163\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39mdata, __unsafe_skip_copy__\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the initial train_state object\n",
    "state = init_train_state(model=mlp, \n",
    "                         random_key=jax.random.PRNGKey(config.jax_seed), \n",
    "                         shape=(1, 785), \n",
    "                         learning_rate=config.learning_rate)\n",
    "\n",
    "prng = jax.random.PRNGKey(21)\n",
    "state, best_state = train_and_evaluate(config.batch_size, \n",
    "                                       state, \n",
    "                                       config.epochs, \n",
    "                                       ckpt_dir='saved_models/',\n",
    "                                       prng=default_rng(seed=np.asarray(prng)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cf526-b817-452c-8654-22f6d3d533e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_state = checkpoints.restore_checkpoint(ckpt_dir='saved_models/', target=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf5ea6-5fdd-4523-b918-75e69fcff081",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PFGM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e151eef46bbff781a0688feac4f2b8ea3cd639d70f4001d0c6cef598b400d9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
