{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3d4f19-f680-4975-9c05-53505381d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 13:47:20.350305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/22.5/math_libs/11.7/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2023-03-23 13:47:20.350397: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/22.5/math_libs/11.7/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2023-03-23 13:47:20.350402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "from jax import random\n",
    "\n",
    "import optax \n",
    "import make_dataset as mkds\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import wandb\n",
    "\n",
    "import make_dataset as mkds\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b444d4a-e30f-404c-b0b9-a2963f420513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class PerturbMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset class that stores the data and targets as NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        data: np.ndarray\n",
    "            The perturbed input data.\n",
    "        targets: np.ndarray\n",
    "            The empirical field that generated the perturbed data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the i-th sample and corresponding target in the dataset.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            idx: int\n",
    "                The index of the sample to return.\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple: A tuple containing the sample and target.\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "    \n",
    "    \n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def create_data_loaders(*datasets : Sequence[data.Dataset],\n",
    "                        train : Union[bool, Sequence[bool]] = True,\n",
    "                        batch_size : int = 128,\n",
    "                        num_workers : int = 4,\n",
    "                        seed : int = 42):\n",
    "    \"\"\"\n",
    "    Creates data loaders used in JAX for a set of datasets.\n",
    "\n",
    "    Args:\n",
    "      datasets: Datasets for which data loaders are created.\n",
    "      train: Sequence indicating which datasets are used for\n",
    "        training and which not. If single bool, the same value\n",
    "        is used for all datasets.\n",
    "      batch_size: Batch size to use in the data loaders.\n",
    "      num_workers: Number of workers for each dataset.\n",
    "      seed: Seed to initialize the workers and shuffling with.\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "    if not isinstance(train, (list, tuple)):\n",
    "        train = [train for _ in datasets]\n",
    "    for dataset, is_train in zip(datasets, train):\n",
    "        loader = data.DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=is_train,\n",
    "                                 drop_last=is_train,\n",
    "                                 collate_fn=numpy_collate,\n",
    "                                 num_workers=num_workers,\n",
    "                                 persistent_workers=is_train,\n",
    "                                 generator=torch.Generator().manual_seed(seed))\n",
    "        loaders.append(loader)\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fb6748-dc2e-4359-abd2-cf41e6ca2230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perturbed_training = mkds.load_data(data_dir='/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_data/MNIST/perturbed/partitioned',\n",
    "                                     data_file='partitioned_training_set.pkl')\n",
    "\n",
    "perturbed_val = mkds.load_data(data_dir='/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_data/MNIST/perturbed/partitioned',\n",
    "                                     data_file='partitioned_val_set.pkl')\n",
    "\n",
    "perturbed_test = mkds.load_data(data_dir='/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_data/MNIST/perturbed/partitioned',\n",
    "                                     data_file='partitioned_test_set.pkl')\n",
    "\n",
    "\n",
    "\n",
    "training = PerturbMNIST(perturbed_training[0], perturbed_training[1])\n",
    "val = PerturbMNIST(perturbed_val[0], perturbed_val[1])\n",
    "testing = PerturbMNIST(perturbed_test[0], perturbed_test[1])\n",
    "\n",
    "train_dl, val_dl, test_dl = create_data_loaders(training, val, testing,\n",
    "                                                train=[True, False, False],\n",
    "                                                batch_size=128)\n",
    "\n",
    "# train_dl = DataLoader(training, \n",
    "#                       collate_fn=mkds.numpy_collate,\n",
    "#                       batch_size=128) # create your dataloader\n",
    "\n",
    "# val_dl = DataLoader(val,\n",
    "#                     collate_fn=mkds.numpy_collate,\n",
    "#                     batch_size=128) # create your dataloader\n",
    "\n",
    "# test_dl = DataLoader(testing, \n",
    "#                      collate_fn=mkds.numpy_collate,\n",
    "#                      batch_size=128) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a83ccbb-51bb-4922-810b-974ca01b678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0a408-f27b-42ed-8db4-cf24426acbb0",
   "metadata": {},
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949474eb-5c09-4033-ac23-7e6e622d4c80",
   "metadata": {},
   "source": [
    "# 2. Make training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07411c66-163e-46e9-bd63-2b039d76b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state, checkpoints\n",
    "from flax.serialization import (\n",
    "    to_state_dict, msgpack_serialize, from_bytes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c516eb14-4401-4a79-8834-e88ec8aabd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    # A simple extension of TrainState to also include batch statistics\n",
    "    # If a model has no batch statistics, it is None\n",
    "    batch_stats : Any = None\n",
    "    # You can further extend the TrainState by any additional part here\n",
    "    # For example, rng to keep for init, dropout, etc.\n",
    "    rng : Any = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92b4a264-4ec8-4250-86f2-f2a317248262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerModule:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_class : nn.Module,\n",
    "                 model_hparams : Dict[str, Any],\n",
    "                 optimizer_hparams : Dict[str, Any],\n",
    "                 exmp_input : Any,\n",
    "                 seed : int = 42,\n",
    "                 logger_params : Dict[str, Any] = None,\n",
    "                 enable_progress_bar : bool = True,\n",
    "                 debug : bool = False,\n",
    "                 check_val_every_n_epoch : int = 1,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A basic Trainer module summarizing most common training functionalities\n",
    "        like logging, model initialization, training loop, etc.\n",
    "\n",
    "        Atributes:\n",
    "          model_class: The class of the model that should be trained.\n",
    "          model_hparams: A dictionary of all hyperparameters of the model. Is\n",
    "            used as input to the model when created.\n",
    "          optimizer_hparams: A dictionary of all hyperparameters of the optimizer.\n",
    "            Used during initialization of the optimizer.\n",
    "          exmp_input: Input to the model for initialization and tabulate.\n",
    "          seed: Seed to initialize PRNG.\n",
    "          logger_params: A dictionary containing the specification of the logger.\n",
    "          enable_progress_bar: If False, no progress bar is shown.\n",
    "          debug: If True, no jitting is applied. Can be helpful for debugging.\n",
    "          check_val_every_n_epoch: The frequency with which the model is evaluated\n",
    "            on the validation set.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model_class = model_class\n",
    "        self.model_hparams = model_hparams\n",
    "        self.optimizer_hparams = optimizer_hparams\n",
    "        self.enable_progress_bar = enable_progress_bar\n",
    "        self.debug = debug\n",
    "        self.seed = seed\n",
    "        self.check_val_every_n_epoch = check_val_every_n_epoch\n",
    "        self.exmp_input = exmp_input\n",
    "        # Set of hyperparameters to save\n",
    "        self.config = {\n",
    "            'model_class': model_class.__name__,\n",
    "            'model_hparams': model_hparams,\n",
    "            'optimizer_hparams': optimizer_hparams,\n",
    "            'logger_params': logger_params,\n",
    "            'enable_progress_bar': self.enable_progress_bar,\n",
    "            'debug': self.debug,\n",
    "            'check_val_every_n_epoch': check_val_every_n_epoch,\n",
    "            'seed': self.seed\n",
    "        }\n",
    "        self.config.update(kwargs)\n",
    "        # Create empty model. Note: no parameters yet\n",
    "        self.model = self.model_class(**self.model_hparams)\n",
    "        self.print_tabulate(exmp_input)\n",
    "        # Init trainer parts\n",
    "        self.init_logger(logger_params)\n",
    "        self.create_jitted_functions()\n",
    "        self.init_model(exmp_input)\n",
    "\n",
    "    def init_logger(self,\n",
    "                    logger_params : Optional[Dict] = None):\n",
    "        \"\"\"\n",
    "        Initializes a logger and creates a logging directory.\n",
    "\n",
    "        Args:\n",
    "          logger_params: A dictionary containing the specification of the logger.\n",
    "        \"\"\"\n",
    "        if logger_params is None:\n",
    "            logger_params = dict()\n",
    "        # Determine logging directory\n",
    "        log_dir = logger_params.get('log_dir', None)\n",
    "        if not log_dir:\n",
    "            base_log_dir = logger_params.get('base_log_dir', 'checkpoints/')\n",
    "            # Prepare logging\n",
    "            log_dir = os.path.join(base_log_dir, self.config[\"model_class\"])\n",
    "            if 'logger_name' in logger_params:\n",
    "                log_dir = os.path.join(log_dir, logger_params['logger_name'])\n",
    "            version = None\n",
    "        else:\n",
    "            version = ''\n",
    "        # Create logger object\n",
    "        logger_type = logger_params.get('logger_type', 'TensorBoard').lower()\n",
    "        if logger_type == 'tensorboard':\n",
    "            self.logger = TensorBoardLogger(save_dir=log_dir,\n",
    "                                            version=version,\n",
    "                                            name='')\n",
    "            log_dir = self.logger.log_dir\n",
    "\n",
    "        elif logger_type == 'wandb':\n",
    "            self.logger = WandbLogger(name=logger_params.get('project_name', None),\n",
    "                                      save_dir=log_dir,\n",
    "                                      version=version,\n",
    "                                      config=self.config)\n",
    "            log_dir = self.logger.save_dir\n",
    "\n",
    "        else:\n",
    "            assert False, f'Unknown logger type \\\"{logger_type}\\\"'\n",
    "        # Save hyperparameters\n",
    "        log_dir = self.logger.save_dir #wandb\n",
    "        print('log_dir =', log_dir)\n",
    "        # log_dir = self.logger.log_dir # tensorboard\n",
    "        if not os.path.isfile(os.path.join(log_dir, 'hparams.json')):\n",
    "            os.makedirs(os.path.join(log_dir, 'metrics/'), exist_ok=True)\n",
    "            with open(os.path.join(log_dir, 'hparams.json'), 'w') as f:\n",
    "                json.dump(self.config, f, indent=4)\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "    def init_model(self,\n",
    "                   exmp_input : Any):\n",
    "        \"\"\"\n",
    "        Creates an initial training state with newly generated network parameters.\n",
    "\n",
    "        Args:\n",
    "          exmp_input: An input to the model with which the shapes are inferred.\n",
    "        \"\"\"\n",
    "        # Prepare PRNG and input\n",
    "        model_rng = random.PRNGKey(self.seed)\n",
    "        model_rng, init_rng = jax.random.split(model_rng)\n",
    "        exmp_input = [exmp_input] if not isinstance(exmp_input, (list, tuple)) else exmp_input\n",
    "        # Run model initialization\n",
    "        variables = self.run_model_init(exmp_input, init_rng)\n",
    "        # Create default state. Optimizer is initialized later\n",
    "        self.state = TrainState(step=0,\n",
    "                                apply_fn=self.model.apply,\n",
    "                                params=variables['params'],\n",
    "                                batch_stats=variables.get('batch_stats'),\n",
    "                                rng=model_rng,\n",
    "                                tx=None,\n",
    "                                opt_state=None)\n",
    "\n",
    "    def run_model_init(self,\n",
    "                       exmp_input : Any,\n",
    "                       init_rng : Any) -> Dict:\n",
    "        \"\"\"\n",
    "        The model initialization call\n",
    "\n",
    "        Args:\n",
    "          exmp_input: An input to the model with which the shapes are inferred.\n",
    "          init_rng: A jax.random.PRNGKey.\n",
    "\n",
    "        Returns:\n",
    "          The initialized variable dictionary.\n",
    "        \"\"\"\n",
    "        return self.model.init(init_rng, *exmp_input, train=True)\n",
    "\n",
    "    def print_tabulate(self,\n",
    "                       exmp_input : Any):\n",
    "        \"\"\"\n",
    "        Prints a summary of the Module represented as table.\n",
    "\n",
    "        Args:\n",
    "          exmp_input: An input to the model with which the shapes are inferred.\n",
    "        \"\"\"\n",
    "        print(self.model.tabulate(random.PRNGKey(0), *exmp_input, train=True))\n",
    "\n",
    "    def init_optimizer(self,\n",
    "                       num_epochs : int,\n",
    "                       num_steps_per_epoch : int):\n",
    "        \"\"\"\n",
    "        Initializes the optimizer and learning rate scheduler.\n",
    "\n",
    "        Args:\n",
    "          num_epochs: Number of epochs the model will be trained for.\n",
    "          num_steps_per_epoch: Number of training steps per epoch.\n",
    "        \"\"\"\n",
    "        hparams = copy(self.optimizer_hparams)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        optimizer_name = hparams.pop('optimizer', 'adamw')\n",
    "        if optimizer_name.lower() == 'adam':\n",
    "            opt_class = optax.adam\n",
    "        elif optimizer_name.lower() == 'adamw':\n",
    "            opt_class = optax.adamw\n",
    "        elif optimizer_name.lower() == 'sgd':\n",
    "            opt_class = optax.sgd\n",
    "        else:\n",
    "            assert False, f'Unknown optimizer \"{opt_class}\"'\n",
    "        # Initialize learning rate scheduler\n",
    "        # A cosine decay scheduler is used, but others are also possible\n",
    "        lr = hparams.pop('lr', 1e-3)\n",
    "        warmup = hparams.pop('warmup', 0)\n",
    "        lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "            init_value=0.0,\n",
    "            peak_value=lr,\n",
    "            warmup_steps=warmup,\n",
    "            decay_steps=int(num_epochs * num_steps_per_epoch),\n",
    "            end_value=0.01 * lr\n",
    "        )\n",
    "        # Clip gradients at max value, and evt. apply weight decay\n",
    "        transf = [optax.clip_by_global_norm(hparams.pop('gradient_clip', 1.0))]\n",
    "        if opt_class == optax.sgd and 'weight_decay' in hparams:  # wd is integrated in adamw\n",
    "            transf.append(optax.add_decayed_weights(hparams.pop('weight_decay', 0.0)))\n",
    "        optimizer = optax.chain(\n",
    "            *transf,\n",
    "            opt_class(lr_schedule, **hparams)\n",
    "        )\n",
    "        # Initialize training state\n",
    "        self.state = TrainState.create(apply_fn=self.state.apply_fn,\n",
    "                                       params=self.state.params,\n",
    "                                       batch_stats=self.state.batch_stats,\n",
    "                                       tx=optimizer,\n",
    "                                       rng=self.state.rng)\n",
    "\n",
    "    def create_jitted_functions(self):\n",
    "        \"\"\"\n",
    "        Creates jitted versions of the training and evaluation functions.\n",
    "        If self.debug is True, not jitting is applied.\n",
    "        \"\"\"\n",
    "        train_step, eval_step = self.create_functions()\n",
    "        if self.debug:  # Skip jitting\n",
    "            print('Skipping jitting due to debug=True')\n",
    "            self.train_step = train_step\n",
    "            self.eval_step = eval_step\n",
    "        else:\n",
    "            self.train_step = jax.jit(train_step)\n",
    "            self.eval_step = jax.jit(eval_step)\n",
    "\n",
    "    def create_functions(self) -> Tuple[Callable[[TrainState, Any], Tuple[TrainState, Dict]],\n",
    "                                        Callable[[TrainState, Any], Tuple[TrainState, Dict]]]:\n",
    "        \"\"\"\n",
    "        Creates and returns functions for the training and evaluation step. The\n",
    "        functions take as input the training state and a batch from the train/\n",
    "        val/test loader. Both functions are expected to return a dictionary of\n",
    "        logging metrics, and the training function a new train state. This\n",
    "        function needs to be overwritten by a subclass. The train_step and\n",
    "        eval_step functions here are examples for the signature of the functions.\n",
    "        \"\"\"\n",
    "        def train_step(state : TrainState,\n",
    "                       batch : Any):\n",
    "            metrics = {}\n",
    "            return state, metrics\n",
    "        def eval_step(state : TrainState,\n",
    "                      batch : Any):\n",
    "            metrics = {}\n",
    "            return metrics\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_model(self,\n",
    "                    train_loader : Iterator,\n",
    "                    val_loader : Iterator,\n",
    "                    test_loader : Optional[Iterator] = None,\n",
    "                    num_epochs : int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Starts a training loop for the given number of epochs.\n",
    "\n",
    "        Args:\n",
    "          train_loader: Data loader of the training set.\n",
    "          val_loader: Data loader of the validation set.\n",
    "          test_loader: If given, best model will be evaluated on the test set.\n",
    "          num_epochs: Number of epochs for which to train the model.\n",
    "\n",
    "        Returns:\n",
    "          A dictionary of the train, validation and evt. test metrics for the\n",
    "          best model on the validation set.\n",
    "        \"\"\"\n",
    "        # Create optimizer and the scheduler for the given number of epochs\n",
    "        self.init_optimizer(num_epochs, len(train_loader))\n",
    "        # Prepare training loop\n",
    "        self.on_training_start()\n",
    "        best_eval_metrics = None\n",
    "        for epoch_idx in self.tracker(range(1, num_epochs+1), desc='Epochs'):\n",
    "            train_metrics = self.train_epoch(train_loader)\n",
    "            self.logger.log_metrics(train_metrics, step=epoch_idx)\n",
    "            self.on_training_epoch_end(epoch_idx)\n",
    "            # Validation every N epochs\n",
    "            if epoch_idx % self.check_val_every_n_epoch == 0:\n",
    "                eval_metrics = self.eval_model(val_loader, log_prefix='val/')\n",
    "                self.on_validation_epoch_end(epoch_idx, eval_metrics, val_loader)\n",
    "                self.logger.log_metrics(eval_metrics, step=epoch_idx)\n",
    "                self.save_metrics(f'eval_epoch_{str(epoch_idx).zfill(3)}', eval_metrics)\n",
    "                # Save best model\n",
    "                if self.is_new_model_better(eval_metrics, best_eval_metrics):\n",
    "                    best_eval_metrics = eval_metrics\n",
    "                    best_eval_metrics.update(train_metrics)\n",
    "                    self.save_model(step=epoch_idx)\n",
    "                    self.save_metrics('best_eval', eval_metrics)\n",
    "        # Test best model if possible\n",
    "        if test_loader is not None:\n",
    "            self.load_model()\n",
    "            test_metrics = self.eval_model(test_loader, log_prefix='test/')\n",
    "            self.logger.log_metrics(test_metrics, step=epoch_idx)\n",
    "            self.save_metrics('test', test_metrics)\n",
    "            best_eval_metrics.update(test_metrics)\n",
    "        # Close logger\n",
    "        self.logger.finalize('success')\n",
    "        wandb.finish()\n",
    "        return best_eval_metrics\n",
    "\n",
    "    def train_epoch(self,\n",
    "                    train_loader : Iterator) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Trains a model for one epoch.\n",
    "\n",
    "        Args:\n",
    "          train_loader: Data loader of the training set.\n",
    "\n",
    "        Returns:\n",
    "          A dictionary of the average training metrics over all batches\n",
    "          for logging.\n",
    "        \"\"\"\n",
    "        # Train model for one epoch, and log avg loss and accuracy\n",
    "        metrics = defaultdict(float)\n",
    "        num_train_steps = len(train_loader)\n",
    "        start_time = time.time()\n",
    "        for batch in self.tracker(train_loader, desc='Training', leave=False):\n",
    "            self.state, step_metrics = self.train_step(self.state, batch)\n",
    "            for key in step_metrics:\n",
    "                metrics['train/' + key] += step_metrics[key] / num_train_steps\n",
    "        metrics = {key: metrics[key].item() for key in metrics}\n",
    "        metrics['epoch_time'] = time.time() - start_time\n",
    "        return metrics\n",
    "\n",
    "    def eval_model(self,\n",
    "                   data_loader : Iterator,\n",
    "                   log_prefix : Optional[str] = '') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a dataset.\n",
    "\n",
    "        Args:\n",
    "          data_loader: Data loader of the dataset to evaluate on.\n",
    "          log_prefix: Prefix to add to all metrics (e.g. 'val/' or 'test/')\n",
    "\n",
    "        Returns:\n",
    "          A dictionary of the evaluation metrics, averaged over data points\n",
    "          in the dataset.\n",
    "        \"\"\"\n",
    "        # Test model on all images of a data loader and return avg loss\n",
    "        metrics = defaultdict(float)\n",
    "        num_elements = 0\n",
    "        for batch in data_loader:\n",
    "            step_metrics = self.eval_step(self.state, batch)\n",
    "            batch_size = batch[0].shape[0] if isinstance(batch, (list, tuple)) else batch.shape[0]\n",
    "            for key in step_metrics:\n",
    "                metrics[key] += step_metrics[key] * batch_size\n",
    "            num_elements += batch_size\n",
    "        metrics = {(log_prefix + key): (metrics[key] / num_elements).item() for key in metrics}\n",
    "        return metrics\n",
    "\n",
    "    def is_new_model_better(self,\n",
    "                            new_metrics : Dict[str, Any],\n",
    "                            old_metrics : Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Compares two sets of evaluation metrics to decide whether the\n",
    "        new model is better than the previous ones or not.\n",
    "\n",
    "        Args:\n",
    "          new_metrics: A dictionary of the evaluation metrics of the new model.\n",
    "          old_metrics: A dictionary of the evaluation metrics of the previously\n",
    "            best model, i.e. the one to compare to.\n",
    "\n",
    "        Returns:\n",
    "          True if the new model is better than the old one, and False otherwise.\n",
    "        \"\"\"\n",
    "        if old_metrics is None:\n",
    "            return True\n",
    "        for key, is_larger in [('val/val_metric', False), ('val/acc', True), ('val/loss', False)]:\n",
    "            if key in new_metrics:\n",
    "                if is_larger:\n",
    "                    return new_metrics[key] > old_metrics[key]\n",
    "                else:\n",
    "                    return new_metrics[key] < old_metrics[key]\n",
    "        assert False, f'No known metrics to log on: {new_metrics}'\n",
    "\n",
    "    def tracker(self,\n",
    "                iterator : Iterator,\n",
    "                **kwargs) -> Iterator:\n",
    "        \"\"\"\n",
    "        Wraps an iterator in a progress bar tracker (tqdm) if the progress bar\n",
    "        is enabled.\n",
    "\n",
    "        Args:\n",
    "          iterator: Iterator to wrap in tqdm.\n",
    "          kwargs: Additional arguments to tqdm.\n",
    "\n",
    "        Returns:\n",
    "          Wrapped iterator if progress bar is enabled, otherwise same iterator\n",
    "          as input.\n",
    "        \"\"\"\n",
    "        if self.enable_progress_bar:\n",
    "            return tqdm(iterator, **kwargs)\n",
    "        else:\n",
    "            return iterator\n",
    "\n",
    "    def save_metrics(self,\n",
    "                     filename : str,\n",
    "                     metrics : Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Saves a dictionary of metrics to file. Can be used as a textual\n",
    "        representation of the validation performance for checking in the terminal.\n",
    "\n",
    "        Args:\n",
    "          filename: Name of the metrics file without folders and postfix.\n",
    "          metrics: A dictionary of metrics to save in the file.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.log_dir, f'metrics/{filename}.json'), 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "\n",
    "    def on_training_start(self):\n",
    "        \"\"\"\n",
    "        Method called before training is started. Can be used for additional\n",
    "        initialization operations etc.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_training_epoch_end(self,\n",
    "                              epoch_idx : int):\n",
    "        \"\"\"\n",
    "        Method called at the end of each training epoch. Can be used for additional\n",
    "        logging or similar.\n",
    "\n",
    "        Args:\n",
    "          epoch_idx: Index of the training epoch that has finished.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_validation_epoch_end(self,\n",
    "                                epoch_idx : int,\n",
    "                                eval_metrics : Dict[str, Any],\n",
    "                                val_loader : Iterator):\n",
    "        \"\"\"\n",
    "        Method called at the end of each validation epoch. Can be used for additional\n",
    "        logging and evaluation.\n",
    "\n",
    "        Args:\n",
    "          epoch_idx: Index of the training epoch at which validation was performed.\n",
    "          eval_metrics: A dictionary of the validation metrics. New metrics added to\n",
    "            this dictionary will be logged as well.\n",
    "          val_loader: Data loader of the validation set, to support additional\n",
    "            evaluation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save_model(self,\n",
    "                   step : int = 0):\n",
    "        \"\"\"\n",
    "        Saves current training state at certain training iteration. Only the model\n",
    "        parameters and batch statistics are saved to reduce memory footprint. To\n",
    "        support the training to be continued from a checkpoint, this method can be\n",
    "        extended to include the optimizer state as well.\n",
    "\n",
    "        Args:\n",
    "          step: Index of the step to save the model at, e.g. epoch.\n",
    "        \"\"\"\n",
    "        checkpoints.save_checkpoint(ckpt_dir=self.log_dir,\n",
    "                                    target={'params': self.state.params,\n",
    "                                            'batch_stats': self.state.batch_stats},\n",
    "                                    step=step,\n",
    "                                    overwrite=True)\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads model parameters and batch statistics from the logging directory.\n",
    "        \"\"\"\n",
    "        state_dict = checkpoints.restore_checkpoint(ckpt_dir=self.log_dir, target=None)\n",
    "        self.state = TrainState.create(apply_fn=self.model.apply,\n",
    "                                       params=state_dict['params'],\n",
    "                                       batch_stats=state_dict['batch_stats'],\n",
    "                                       # Optimizer will be overwritten when training starts\n",
    "                                       tx=self.state.tx if self.state.tx else optax.sgd(0.1),\n",
    "                                       rng=self.state.rng\n",
    "                                      )\n",
    "\n",
    "    def bind_model(self):\n",
    "        \"\"\"\n",
    "        Returns a model with parameters bound to it. Enables an easier inference\n",
    "        access.\n",
    "\n",
    "        Returns:\n",
    "          The model with parameters and evt. batch statistics bound to it.\n",
    "        \"\"\"\n",
    "        params = {'params': self.state.params}\n",
    "        if self.state.batch_stats:\n",
    "            params['batch_stats'] = self.state.batch_stats\n",
    "        return self.model.bind(params)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls,\n",
    "                             checkpoint : str,\n",
    "                             exmp_input : Any) -> Any:\n",
    "        \"\"\"\n",
    "        Creates a Trainer object with same hyperparameters and loaded model from\n",
    "        a checkpoint directory.\n",
    "\n",
    "        Args:\n",
    "          checkpoint: Folder in which the checkpoint and hyperparameter file is stored.\n",
    "          exmp_input: An input to the model for shape inference.\n",
    "\n",
    "        Returns:\n",
    "          A Trainer object with model loaded from the checkpoint folder.\n",
    "        \"\"\"\n",
    "        hparams_file = os.path.join(checkpoint, 'hparams.json')\n",
    "        assert os.path.isfile(hparams_file), 'Could not find hparams file'\n",
    "        with open(hparams_file, 'r') as f:\n",
    "            hparams = json.load(f)\n",
    "        hparams.pop('model_class')\n",
    "        hparams.update(hparams.pop('model_hparams'))\n",
    "        if not hparams['logger_params']:\n",
    "            hparams['logger_params'] = dict()\n",
    "        hparams['logger_params']['log_dir'] = checkpoint\n",
    "        trainer = cls(exmp_input=exmp_input,\n",
    "                      **hparams)\n",
    "        trainer.load_model()\n",
    "        return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e76217-c5ca-4da6-baef-8c3394edbf56",
   "metadata": {},
   "source": [
    "## 1. Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8688a5d-c6c5-4f2a-b16a-f935a7cf2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    hidden_dims : Sequence[int]\n",
    "    output_dim : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, **kwargs):\n",
    "        for dims in self.hidden_dims:\n",
    "            x = nn.Dense(dims)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPRegressTrainer(TrainerModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_dims : Sequence[int],\n",
    "                 output_dim : int,\n",
    "                 **kwargs):\n",
    "        super().__init__(model_class=MLPRegressor,\n",
    "                         model_hparams={\n",
    "                             'hidden_dims': hidden_dims,\n",
    "                             'output_dim': output_dim\n",
    "                         },\n",
    "                         **kwargs)\n",
    "\n",
    "    def create_functions(self):\n",
    "        def mse_loss(params, batch):\n",
    "            x, y = batch\n",
    "            pred = self.model.apply({'params': params}, x)\n",
    "            loss = ((pred - y) ** 2).mean()\n",
    "            return loss\n",
    "\n",
    "        def train_step(state, batch):\n",
    "            loss_fn = lambda params: mse_loss(params, batch)\n",
    "            loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "            state = state.apply_gradients(grads=grads)\n",
    "            metrics = {'loss': loss}\n",
    "            return state, metrics\n",
    "\n",
    "        def eval_step(state, batch):\n",
    "            loss = mse_loss(state.params, batch)\n",
    "            return {'loss': loss}\n",
    "\n",
    "        return train_step, eval_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b7c075-6103-4c6f-97b7-75abe9d0da85",
   "metadata": {},
   "source": [
    "Commented code trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e8550fa-f013-4a15-b2fb-11c1e821ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = 'PFGM_MNIST/saved_models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1968e768-9303-48ab-a32a-5344d9d68207",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = create_data_loaders(training, val, testing,\n",
    "                                                train=[True, False, False],\n",
    "                                                batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fca88f-8eda-429d-8e59-a808f2878883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                              MLPRegressor Summary                              \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams         \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
      "│         │ MLPRegressor │ -               │ \u001b[2mfloat32\u001b[0m[64,785] │                 │\n",
      "│         │              │ \u001b[2mfloat32\u001b[0m[64,785] │                 │                 │\n",
      "│         │              │ - train: True   │                 │                 │\n",
      "├─────────┼──────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
      "│ Dense_0 │ Dense        │ \u001b[2mfloat32\u001b[0m[64,785] │ \u001b[2mfloat32\u001b[0m[64,785] │ bias:           │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785]    │\n",
      "│         │              │                 │                 │ kernel:         │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785,78… │\n",
      "│         │              │                 │                 │                 │\n",
      "│         │              │                 │                 │ \u001b[1m617,010 \u001b[0m\u001b[1;2m(2.5 \u001b[0m   │\n",
      "│         │              │                 │                 │ \u001b[1;2mMB)\u001b[0m             │\n",
      "├─────────┼──────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
      "│ Dense_1 │ Dense        │ \u001b[2mfloat32\u001b[0m[64,785] │ \u001b[2mfloat32\u001b[0m[64,785] │ bias:           │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785]    │\n",
      "│         │              │                 │                 │ kernel:         │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785,78… │\n",
      "│         │              │                 │                 │                 │\n",
      "│         │              │                 │                 │ \u001b[1m617,010 \u001b[0m\u001b[1;2m(2.5 \u001b[0m   │\n",
      "│         │              │                 │                 │ \u001b[1;2mMB)\u001b[0m             │\n",
      "├─────────┼──────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
      "│ Dense_2 │ Dense        │ \u001b[2mfloat32\u001b[0m[64,785] │ \u001b[2mfloat32\u001b[0m[64,785] │ bias:           │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785]    │\n",
      "│         │              │                 │                 │ kernel:         │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785,78… │\n",
      "│         │              │                 │                 │                 │\n",
      "│         │              │                 │                 │ \u001b[1m617,010 \u001b[0m\u001b[1;2m(2.5 \u001b[0m   │\n",
      "│         │              │                 │                 │ \u001b[1;2mMB)\u001b[0m             │\n",
      "├─────────┼──────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
      "│ Dense_3 │ Dense        │ \u001b[2mfloat32\u001b[0m[64,785] │ \u001b[2mfloat32\u001b[0m[64,785] │ bias:           │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785]    │\n",
      "│         │              │                 │                 │ kernel:         │\n",
      "│         │              │                 │                 │ \u001b[2mfloat32\u001b[0m[785,78… │\n",
      "│         │              │                 │                 │                 │\n",
      "│         │              │                 │                 │ \u001b[1m617,010 \u001b[0m\u001b[1;2m(2.5 \u001b[0m   │\n",
      "│         │              │                 │                 │ \u001b[1;2mMB)\u001b[0m             │\n",
      "├─────────┼──────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2,468,040 \u001b[0m\u001b[1;2m(9.9 \u001b[0m\u001b[1m \u001b[0m│\n",
      "│\u001b[1m         \u001b[0m│\u001b[1m              \u001b[0m│\u001b[1m                 \u001b[0m│\u001b[1m                 \u001b[0m│\u001b[1m \u001b[0m\u001b[1;2mMB)\u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴──────────────┴─────────────────┴─────────────────┴─────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                      Total Parameters: 2,468,040 \u001b[0m\u001b[1;2m(9.9 MB)\u001b[0m\u001b[1m                      \u001b[0m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdowicz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>PFGM_MNIST/saved_models/MLPRegressor/wandb/run-20230323_134727-w1y03qhk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mdowicz/lightning_logs/runs/w1y03qhk' target=\"_blank\">checking_best_run</a></strong> to <a href='https://wandb.ai/mdowicz/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mdowicz/lightning_logs' target=\"_blank\">https://wandb.ai/mdowicz/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mdowicz/lightning_logs/runs/w1y03qhk' target=\"_blank\">https://wandb.ai/mdowicz/lightning_logs/runs/w1y03qhk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir = PFGM_MNIST/saved_models/MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 13:47:29.051670: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2163] Execution of replica 0 failed: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mMLPRegressTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m785\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m785\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m785\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m785\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43moptimizer_hparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                               \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlogger_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbase_log_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogger_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwandb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                           \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproject_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchecking_best_run\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mexmp_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mcheck_val_every_n_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# metrics = trainer.train_model(train_dl,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#                               val_dl,\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#                               test_loader=test_dl,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(f'Validation loss: {metrics[\"val/loss\"]}')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(f'Test loss: {metrics[\"test/loss\"]}')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m, in \u001b[0;36mMLPRegressTrainer.__init__\u001b[0;34m(self, hidden_dims, output_dim, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     17\u001b[0m              hidden_dims : Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m     18\u001b[0m              output_dim : \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     19\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMLPRegressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmodel_hparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dims\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                     \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 59\u001b[0m, in \u001b[0;36mTrainerModule.__init__\u001b[0;34m(self, model_class, model_hparams, optimizer_hparams, exmp_input, seed, logger_params, enable_progress_bar, debug, check_val_every_n_epoch, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_logger(logger_params)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_jitted_functions()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexmp_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 119\u001b[0m, in \u001b[0;36mTrainerModule.init_model\u001b[0;34m(self, exmp_input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Prepare PRNG and input\u001b[39;00m\n\u001b[1;32m    118\u001b[0m model_rng \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m--> 119\u001b[0m model_rng, init_rng \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_rng\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m exmp_input \u001b[38;5;241m=\u001b[39m [exmp_input] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exmp_input, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;28;01melse\u001b[39;00m exmp_input\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Run model initialization\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/random.py:214\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a PRNG key into `num` new keys by adding a leading axis.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m  An array-like object of `num` new PRNG keys.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m key, wrapped \u001b[38;5;241m=\u001b[39m _check_prng_key(key)\n\u001b[0;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(wrapped, \u001b[43m_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/random.py:200\u001b[0m, in \u001b[0;36m_split\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m    198\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit accepts a single key, but was given a key array of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/prng.py:606\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(keys, count)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split\u001b[39m(keys, count):\n\u001b[0;32m--> 606\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_split_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/core.py:329\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    327\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_enable_checks \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    328\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 329\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/core.py:332\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 332\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/core.py:712\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 712\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/prng.py:618\u001b[0m, in \u001b[0;36mrandom_split_impl\u001b[0;34m(keys, count)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;129m@random_split_p\u001b[39m\u001b[38;5;241m.\u001b[39mdef_impl\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split_impl\u001b[39m(keys, \u001b[38;5;241m*\u001b[39m, count):\n\u001b[0;32m--> 618\u001b[0m   base_arr \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_split_impl_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsafe_raw_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m PRNGKeyArray(keys\u001b[38;5;241m.\u001b[39mimpl, base_arr)\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/prng.py:624\u001b[0m, in \u001b[0;36mrandom_split_impl_base\u001b[0;34m(impl, base_arr, keys_ndim, count)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split_impl_base\u001b[39m(impl, base_arr, keys_ndim, \u001b[38;5;241m*\u001b[39m, count):\n\u001b[1;32m    623\u001b[0m   split \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(keys_ndim, \u001b[38;5;28;01mlambda\u001b[39;00m k: impl\u001b[38;5;241m.\u001b[39msplit(k, count))\n\u001b[0;32m--> 624\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_arr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/prng.py:623\u001b[0m, in \u001b[0;36mrandom_split_impl_base.<locals>.<lambda>\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_split_impl_base\u001b[39m(impl, base_arr, keys_ndim, \u001b[38;5;241m*\u001b[39m, count):\n\u001b[0;32m--> 623\u001b[0m   split \u001b[38;5;241m=\u001b[39m iterated_vmap_unary(keys_ndim, \u001b[38;5;28;01mlambda\u001b[39;00m k: \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    624\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m split(base_arr)\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/_src/prng.py:1108\u001b[0m, in \u001b[0;36mthreefry_split\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _threefry_split_foldlike(key, \u001b[38;5;28mint\u001b[39m(num))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1108\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_threefry_split_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/jax_pfgm/lib/python3.9/site-packages/jax/interpreters/pxla.py:2136\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2134\u001b[0m   out_bufs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_tokens(input_bufs)\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2136\u001b[0m   out_bufs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded_on_local_devices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   2139\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m bufs \u001b[38;5;129;01min\u001b[39;00m out_bufs:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.custom_call' failed: jaxlib/gpu/prng_kernels.cc:33: operation gpuGetLastError() failed: out of memory."
     ]
    }
   ],
   "source": [
    "trainer = MLPRegressTrainer(hidden_dims=[785, 785, 785],\n",
    "                            output_dim=785,\n",
    "                            optimizer_hparams={'lr': 4e-3,\n",
    "                                               'optimizer': 'adam'},\n",
    "                            logger_params={'base_log_dir': CHECKPOINT_PATH,\n",
    "                                           'logger_type': 'wandb',\n",
    "                                           'project_name': 'checking_best_run'},\n",
    "                            exmp_input=next(iter(train_dl))[0:1],\n",
    "                            check_val_every_n_epoch=10)\n",
    "\n",
    "# metrics = trainer.train_model(train_dl,\n",
    "#                               val_dl,\n",
    "#                               test_loader=test_dl,\n",
    "#                               num_epochs=500)\n",
    "\n",
    "# print(f'Training loss: {metrics[\"train/loss\"]}')\n",
    "# print(f'Validation loss: {metrics[\"val/loss\"]}')\n",
    "# print(f'Test loss: {metrics[\"test/loss\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8b933-4cc1-4a49-889b-700cad82c083",
   "metadata": {},
   "source": [
    "Code below reloads the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b867d0-26eb-451a-b0f2-f02d6297df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_dir = '/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_models/MLPRegressor/version_9'\n",
    "# model_dir = '/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_models/wandb/latest-run/run-xmvj9qm1.wandb'\n",
    "model_dir = '/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_models/MLPRegressor/checkpoint_3970'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30cd41-8c11-497a-a114-de112273aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_loader = MLPRegressTrainer.load_from_checkpoint(checkpoint=model_dir,\n",
    "                                                        exmp_input=next(iter(train_dl))[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e6979-cd2a-45ae-9a02-c9a5a8690ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = mkds.download_MNIST()\n",
    "test_data, test_labels = test_ds.data.numpy(), test_ds.targets.numpy()\n",
    "test_data = mkds.reshape_with_channel_dim(test_data)\n",
    "perturbed_test_data = mkds.process_perturbed_data(test_data, jax.random.PRNGKey(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee98ae-fd10-4837-b9e0-26da16214a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_test_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18926090-2622-4499-a737-12aae8e7db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bd = trainer_loader.bind_model()\n",
    "batch = next(iter(test_dl))\n",
    "pred = model_bd(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56887067-effc-487c-84d8-36f2b74af5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batchsize=400\n",
    "sample_idx=0\n",
    "x_coord=12\n",
    "y_coord=8\n",
    "rng = jax.random.PRNGKey(21)\n",
    "grid_lims=40\n",
    "# figure_name: str\n",
    "# figure_dir: str = 'figures/'\n",
    "\n",
    "train_ds, test_ds = mkds.download_MNIST()\n",
    "test_data, test_labels = test_ds.data.numpy(), test_ds.targets.numpy()\n",
    "test_data = mkds.reshape_with_channel_dim(test_data)\n",
    "perturbed_test_data = mkds.process_perturbed_data(test_data, jax.random.PRNGKey(7))\n",
    "\n",
    "vis_batch = perturbed_test_data[0][:batchsize]\n",
    "truth = perturbed_test_data[1][:batchsize]\n",
    "\n",
    "\n",
    "model_bd = trainer_loader.bind_model()\n",
    "pred = model_bd(vis_batch)\n",
    "\n",
    "flat_coord = x_coord * 28 + y_coord\n",
    "everything_except = np.arange(len(vis_batch)) != sample_idx\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,8))\n",
    "ax[0].quiver(vis_batch[everything_except, flat_coord], vis_batch[everything_except,-1],\n",
    "             truth[everything_except, flat_coord], truth[everything_except,-1], label='Poisson Field of other samples');\n",
    "# This quiver plot highlights the selected pixel in the left panel. Allowing the audience to see how \n",
    "# this one pixel is mapped into the N+1 dimension above the hyperplane.\n",
    "ax[0].quiver(vis_batch[sample_idx, flat_coord], vis_batch[sample_idx,-1],\n",
    "             pred[sample_idx, flat_coord], pred[sample_idx,-1], color='red', alpha=0.7, label='This pixels Poisson Field');\n",
    "ax[0].set_title(f'Poisson field for pixel {x_coord, y_coord}')\n",
    "ax[0].set_xlabel(f'Perturbed value of pixel {x_coord, y_coord}')\n",
    "ax[0].set_ylabel(f'Z value of pixel {x_coord, y_coord}')\n",
    "ax[0].legend()\n",
    "ax[1].quiver(vis_batch[everything_except, flat_coord], vis_batch[everything_except,-1],\n",
    "             pred[everything_except, flat_coord], pred[everything_except,-1], label='Poisson Field of other samples');\n",
    "# This quiver plot highlights the selected pixel in the left panel. Allowing the audience to see how \n",
    "# this one pixel is mapped into the N+1 dimension above the hyperplane.\n",
    "ax[1].quiver(vis_batch[sample_idx, flat_coord], vis_batch[sample_idx,-1],\n",
    "             pred[sample_idx, flat_coord], pred[sample_idx,-1], color='red', alpha=0.7, label='This pixels Poisson Field');\n",
    "ax[1].set_title(f'Predicted Poisson field for pixel {x_coord, y_coord}')\n",
    "ax[1].set_xlabel(f'Perturbed value of pixel {x_coord, y_coord}')\n",
    "ax[1].set_ylabel(f'Z value of pixel {x_coord, y_coord}')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1b25c-c6e5-4cf5-9041-df7eb15fdf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFGM",
   "language": "python",
   "name": "jax_pfgm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
