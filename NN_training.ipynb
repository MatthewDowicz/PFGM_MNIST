{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3d4f19-f680-4975-9c05-53505381d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 13:45:40.250066: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/22.5/math_libs/11.7/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2023-03-16 13:45:40.250148: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/22.5/math_libs/11.7/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2023-03-16 13:45:40.250152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "\n",
    "import optax \n",
    "import make_dataset as mkds\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os \n",
    "\n",
    "import make_dataset as mkds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb6748-dc2e-4359-abd2-cf41e6ca2230",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = mkds.load_dataloaders(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0a408-f27b-42ed-8db4-cf24426acbb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949474eb-5c09-4033-ac23-7e6e622d4c80",
   "metadata": {},
   "source": [
    "# 2. Make training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07411c66-163e-46e9-bd63-2b039d76b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state, checkpoints\n",
    "from flax.serialization import (\n",
    "    to_state_dict, msgpack_serialize, from_bytes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e76217-c5ca-4da6-baef-8c3394edbf56",
   "metadata": {},
   "source": [
    "## 1. Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe041904-968c-493b-ab1a-3f862a8eefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP model for testing PFGM.\n",
    "\n",
    "    Due to it's simplicity we use @nn.compact instead of setup\n",
    "    \"\"\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(360)(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dense(785)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ccc54cb-4acc-44d3-a7ff-93ccdf9cdbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                  MLP Summary                                   \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                 \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ MLP    │ \u001b[2mfloat32\u001b[0m[1,785] │ \u001b[2mfloat32\u001b[0m[1,785] │                         │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,785] │ \u001b[2mfloat32\u001b[0m[1,360] │ bias: \u001b[2mfloat32\u001b[0m[360]      │\n",
      "│         │        │                │                │ kernel:                 │\n",
      "│         │        │                │                │ \u001b[2mfloat32\u001b[0m[785,360]        │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m282,960 \u001b[0m\u001b[1;2m(1.1 MB)\u001b[0m        │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,360] │ \u001b[2mfloat32\u001b[0m[1,785] │ bias: \u001b[2mfloat32\u001b[0m[785]      │\n",
      "│         │        │                │                │ kernel:                 │\n",
      "│         │        │                │                │ \u001b[2mfloat32\u001b[0m[360,785]        │\n",
      "│         │        │                │                │                         │\n",
      "│         │        │                │                │ \u001b[1m283,385 \u001b[0m\u001b[1;2m(1.1 MB)\u001b[0m        │\n",
      "├─────────┼────────┼────────────────┼────────────────┼─────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m         Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m566,345 \u001b[0m\u001b[1;2m(2.3 MB)\u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴────────────────┴────────────────┴─────────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                       Total Parameters: 566,345 \u001b[0m\u001b[1;2m(2.3 MB)\u001b[0m\u001b[1m                       \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View model layers\n",
    "mlp = MLP()\n",
    "\n",
    "print(mlp.tabulate(jax.random.PRNGKey(0), jnp.ones((1, 785))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa9be41-f06e-4c99-8a73-2bd7c968a27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 785)\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl, test_dl = mkds.load_dataloaders(batch_size=128)\n",
    "\n",
    "params = mlp.init(jax.random.PRNGKey(0), jnp.ones((1, 785)))['params']\n",
    "data_batch = next(iter(train_dl))\n",
    "pred = mlp.apply({'params': params}, data_batch[0])\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4db6f-23de-413d-9c3d-ccc2972b05d0",
   "metadata": {},
   "source": [
    "## 2. Create a `TrainState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1159f13b-8813-48a0-ab21-c7bae5469e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(model: Any,\n",
    "                     random_key: Any,\n",
    "                     shape: tuple,\n",
    "                     learning_rate: int) -> train_state.TrainState:\n",
    "    \"\"\"\n",
    "    Function to initialize the TrainState dataclass, which represents\n",
    "    the entire training state, including step number, parameters, and \n",
    "    optimizer state. This is useful because we no longer need to\n",
    "    initialize the model again and again with new variables, we just \n",
    "    update the \"state\" of the mdoel and pass this as inputs to functions.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        model: nn.Module    \n",
    "            The model that we want to train.\n",
    "        random_key: jax.random.PRNGKey()\n",
    "            Used to trigger the initialization functions, which generate\n",
    "            the initial set of parameters that the model will use.\n",
    "        shape: tuple\n",
    "            Shape of the batch of data that will be input into the model.\n",
    "            This is used to trigger shape inference, which is where the model\n",
    "            figures out by itself what the correct size the weights should be\n",
    "            when they see the inputs.\n",
    "        learning_rate: int\n",
    "            How large of a step the optimizer should take.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        train_state.TrainState:\n",
    "            A utility class for handling parameter and gradient updates. \n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    variables = model.init(random_key, jnp.ones(shape))\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate) # TODO update this to be user defined\n",
    "\n",
    "    # Create a state\n",
    "    return train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                         tx=optimizer,\n",
    "                                         params=variables['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3b4698-fc3b-4995-b3c2-2de7808759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_rng = jax.random.PRNGKey(0)\n",
    "\n",
    "learning_rate = 0.01\n",
    "state = init_train_state(mlp, init_rng, (1, 785), learning_rate)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ee2f9-8ff7-469b-9e6d-ef41dd580694",
   "metadata": {},
   "source": [
    "## 3. Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395bfe70-0551-4a0e-be55-317ac92b3622",
   "metadata": {},
   "source": [
    "A function that:\n",
    "\n",
    "- Evaluates the neural network given the parameters and a batch of input images with `TrainState.apply_fn` (which contains the `Module.apply` method (forward pass)).\n",
    "\n",
    "- Computes the cross entropy loss, using the predefined `optax.l2_loss`. Note that this function expects integer labels, so there is no need to convert labels to onehot encoding.\n",
    "\n",
    "- Evaluates the gradient of the loss function using jax.grad.\n",
    "\n",
    "- Applies a pytree of gradients to the optimizer to update the model’s parameters.\n",
    "\n",
    "Use JAX’s `@jit` decorator to trace the entire `train_step` function and just-in-time compile it with XLA into fused device operations that run faster and more efficiently on hardware accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c589953-9178-4e12-965e-0d081d2b2476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: train_state.TrainState,\n",
    "               batch: list):\n",
    "    \"\"\"\n",
    "    Function to run training on one batch of data.\n",
    "    \"\"\"\n",
    "    data, targets = batch\n",
    "\n",
    "    def loss_fn(params: dict):\n",
    "        \"\"\"\n",
    "        Simple MSE loss as described in the PFGM paper.\n",
    "        \"\"\"\n",
    "        pred = state.apply_fn({'params': params}, data)\n",
    "        loss = optax.l2_loss(\n",
    "            predictions=pred, targets=targets).mean()\n",
    "        return loss, pred\n",
    "\n",
    "    def r_squared(params):\n",
    "        \"\"\"\n",
    "        Function to calculate the coefficient of determination or \n",
    "        R^2, which quantifies how well the regression model fits \n",
    "        the observed data. Or more formally, it is a statistical\n",
    "        measure that represents the proportion of variance in the\n",
    "        dependent variable that is explained by the independent \n",
    "        variable(s) in a regression model. R^2 ranges from 0 to 1, \n",
    "        with a higher value indicating a better fit. \n",
    "\n",
    "        An R^2 of 0 means that the regression model does not explain\n",
    "        any of the variability in the dependent variable, while an\n",
    "        R^2 of 1 indicates that the regression model explains all of\n",
    "        the variability in the dependent model.\n",
    "        \"\"\"\n",
    "        pred = state.apply_fn({'params': params}, data)\n",
    "        residual = jnp.sum(jnp.square(targets - pred))\n",
    "        total = jnp.sum(jnp.square(targets - jnp.mean(targets)))\n",
    "        r2_score = 1 - (residual / total)\n",
    "        return r2_score\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, pred), grads = gradient_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(pred=pred, targets=targets)\n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    data, targets = batch\n",
    "    pred = state.apply_fn({'params': state.params}, data)\n",
    "    return compute_metrics(pred=pred, targets=targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c14091-f5a9-4260-ab8b-b876de1c6494",
   "metadata": {},
   "source": [
    "## 4. Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c56748e-8305-4d6d-bc0f-c8a0e6b40200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, pred, targets):\n",
    "    \"\"\"\n",
    "    Function that computes metrics that will be logged\n",
    "    during training\n",
    "    \"\"\"\n",
    "    # Calculate the MSE loss\n",
    "    loss = ((pred - targets) ** 2).mean()\n",
    "\n",
    "    # Calculate the R^2 score\n",
    "    residual = jnp.sum(jnp.square(targets - pred))\n",
    "    total = jnp.sum(jnp.square(targets - jnp.mean(targets)))\n",
    "    r2_score = 1 - (residual / total)\n",
    "\n",
    "    # Save these metrics into a dict\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'r2': r2_score\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def accumulate_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Function that accumulates all the metrics for each batch and \n",
    "    accumulates/calculates the metrics for each epoch.\n",
    "    \"\"\"\n",
    "    metrics = jax.device_get(metrics)\n",
    "    return {\n",
    "        k: np.mean([metric[k] for metric in metrics])\n",
    "        for k in metrics[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717df8d-7b22-4d8f-99ee-6beae5a633f8",
   "metadata": {},
   "source": [
    "## 5. Initialize the `TrainState`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b408d4-5f12-49c3-9216-dab77aeba008",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_rng = jax.random.PRNGKey(0)\n",
    "\n",
    "learning_rate = 0.01\n",
    "state = init_train_state(mlp, init_rng, (1, 785), learning_rate)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4afa520-2af8-4fb1-9dea-bf8c38eb95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = mkds.load_dataloaders(batch_size=128)\n",
    "\n",
    "train_batch_metrics = []\n",
    "for cnt, batch in enumerate(train_dl):\n",
    "    state, metrics = train_step(state, batch)\n",
    "    train_batch_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52730114-6c95-4fe9-8c1a-ce13e27ac384",
   "metadata": {},
   "source": [
    "## 6. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "789c1e7a-a0b3-467e-9a4e-b2c0216234ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PerturbMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset class that stores the data and targets as NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        data: np.ndarray\n",
    "            The perturbed input data.\n",
    "        targets: np.ndarray\n",
    "            The empirical field that generated the perturbed data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the i-th sample and corresponding target in the dataset.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            idx: int\n",
    "                The index of the sample to return.\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple: A tuple containing the sample and target.\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efbc68f7-dc58-4c86-b644-fc63415a153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_training = mkds.load_data(data_dir='/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_data/MNIST/perturbed/partitioned',\n",
    "                                     data_file='partitioned_training_set.pkl')\n",
    "\n",
    "perturbed_val = mkds.load_data(data_dir='/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_data/MNIST/perturbed/partitioned',\n",
    "                                     data_file='partitioned_val_set.pkl')\n",
    "\n",
    "perturbed_test = mkds.load_data(data_dir='/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_data/MNIST/perturbed/partitioned',\n",
    "                                     data_file='partitioned_test_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f3f3025-2f23-4536-9780-b21fbf0592aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "training = PerturbMNIST(perturbed_training[0], perturbed_training[1])\n",
    "train_dl = DataLoader(training, \n",
    "                      collate_fn=mkds.numpy_collate,\n",
    "                      batch_size=128) # create your dataloader\n",
    "\n",
    "val = PerturbMNIST(perturbed_val[0], perturbed_val[1])\n",
    "val_dl = DataLoader(val,\n",
    "                    collate_fn=mkds.numpy_collate,\n",
    "                    batch_size=128) # create your dataloader\n",
    "\n",
    "testing = PerturbMNIST(perturbed_test[0], perturbed_test[1])\n",
    "test_dl = DataLoader(testing, \n",
    "                     collate_fn=mkds.numpy_collate,\n",
    "                     batch_size=128) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c855ac9f-6286-4891-98df-fab09f79e3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c6b4a1729e4f8a976d86fe93f8842c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (1/2): Loss: 63102800363520.0000, r2: -63184274718720.00\n",
      "Val (1/2): Loss: 45787794898944.0000, r2: -45846980722688.00\n",
      "Test: Loss: 142529786797883392.0000, r2: -142713448189394944.00\n",
      "\n",
      "TRAIN (2/2): Loss: 48549882494976.0000, r2: -48612545396736.00\n",
      "Val (2/2): Loss: 59777925251072.0000, r2: -59855154970624.00\n",
      "Test: Loss: 134506830838628352.0000, r2: -134680184308629504.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1,epochs+1)):\n",
    "    ### Training ###\n",
    "    train_batch_metrics = []\n",
    "    for cnt, batch in enumerate(train_dl):\n",
    "        state, metrics = train_step(state, batch)\n",
    "        train_batch_metrics.append(metrics)\n",
    "    train_metrics = accumulate_metrics(train_batch_metrics)\n",
    "    \n",
    "    print(\n",
    "        'TRAIN (%d/%d): Loss: %.4f, r2: %.2f' % (\n",
    "            epoch, epochs, train_metrics['loss'], \n",
    "            train_metrics['r2'])\n",
    "    )   \n",
    "    \n",
    "    ### Validation ###\n",
    "    val_batch_metrics = []\n",
    "    for cnt, batch in enumerate(val_dl):\n",
    "        state, metrics = train_step(state, batch)\n",
    "        val_batch_metrics.append(metrics)\n",
    "    val_metrics = accumulate_metrics(val_batch_metrics)\n",
    "    print(\n",
    "        'Val (%d/%d): Loss: %.4f, r2: %.2f' % (\n",
    "            epoch, epochs, val_metrics['loss'], \n",
    "            val_metrics['r2'])\n",
    "    )\n",
    "\n",
    "    ### Testing ###    \n",
    "    test_batch_metrics = []\n",
    "    for cnt, batch in enumerate(test_dl):\n",
    "            metrics = eval_step(state, batch)\n",
    "            test_batch_metrics.append(metrics)\n",
    "        \n",
    "    test_metrics = accumulate_metrics(test_batch_metrics)\n",
    "    print(\n",
    "        'Test: Loss: %.4f, r2: %.2f' % (\n",
    "            test_metrics['loss'],\n",
    "            test_metrics['r2']\n",
    "        )\n",
    "    )\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcde60fb-ade6-4621-916c-e60f58876485",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = '/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "657e3beb-9a26-4be5-8d06-50a05a4c0089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/pscratch/sd/m/mdowicz/PFGM_MNIST/saved_models/checkpoint_0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints.save_checkpoint(ckpt_dir=CHECKPOINT_PATH, target=state, step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4d5d92f-4518-4876-8439-ea8adb364fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_state = checkpoints.restore_checkpoint(ckpt_dir=CHECKPOINT_PATH, target=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff2ab9-9e01-4f7a-bb90-43d9a59b1c99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFGM",
   "language": "python",
   "name": "jax_pfgm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
