{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70138ada-0df7-44e1-bf76-308970b77fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 16:26:01.327452: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/22.5/math_libs/11.7/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2023-03-27 16:26:01.327557: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/nvidia/hpc_sdk/Linux_x86_64/22.5/math_libs/11.7/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/CUPTI/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/extras/Debugger/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/nvvm/lib64:/opt/nvidia/hpc_sdk/Linux_x86_64/22.5/cuda/11.7/lib64:/opt/cray/pe/papi/7.0.0.1/lib64:/opt/cray/pe/gcc/11.2.0/snos/lib64:/opt/cray/libfabric/1.15.2.0/lib64\n",
      "2023-03-27 16:26:01.327562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdowicz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Basic Imports\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing fonts to be latex typesetting\n",
    "from matplotlib import rcParams\n",
    "rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "rcParams['font.family'] = 'serif'\n",
    "\n",
    "# JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "from flax.serialization import (\n",
    "    to_state_dict, msgpack_serialize, from_bytes\n",
    ")\n",
    "import optax\n",
    "\n",
    "# Logging with Tensorboard or Weights and Biases\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "# For ODESolver\n",
    "from scipy import integrate\n",
    "\n",
    "# PyTorch for Dataloaders\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# Wandb \n",
    "import wandb\n",
    "wandb.login()\n",
    "import pprint\n",
    "\n",
    "# Path to import created files\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/src\")\n",
    "\n",
    "# Import created functions\n",
    "import make_dataset as mkds\n",
    "import visualization as vis\n",
    "import flax_trn_loop as trn\n",
    "# import NN_model as nnm\n",
    "# import observable_data as od\n",
    "\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a428b4-d19c-4530-955d-096680953c85",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21ca70b-0edd-48cf-8a4c-70dd38b28106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP model for testing PFGM.\n",
    "\n",
    "    Due to it's simplicity we use @nn.compact instead of setup\n",
    "    \"\"\"\n",
    "    hidden_dims: Sequence[int]\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, **kwargs):\n",
    "        for dim in self.hidden_dims:\n",
    "            x = nn.Dense(dim)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.output_dim)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83b8e68-285a-4c8a-9379-bf8fca7daa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View model layers\n",
    "# mlp = MLP(hidden_dims=config.hidden_dims,\n",
    "#           output_dim=config.output_dim)\n",
    "\n",
    "# print(mlp.tabulate(jax.random.PRNGKey(config.jax_seed), jnp.ones((1, 785))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9333f-76cd-4a15-aea2-3e9c51d2e99e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Create the `train_state` that will be passed between updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b113541-f6ff-45b8-acaf-ab93fd49dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_state(model: Any,\n",
    "                     random_key: Any,\n",
    "                     shape: tuple,\n",
    "                     learning_rate: int) -> train_state.TrainState:\n",
    "    \"\"\"\n",
    "    Function to initialize the TrainState dataclass, which represents\n",
    "    the entire training state, including step number, parameters, and \n",
    "    optimizer state. This is useful because we no longer need to\n",
    "    initialize the model again and again with new variables, we just \n",
    "    update the \"state\" of the mdoel and pass this as inputs to functions.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        model: nn.Module    \n",
    "            The model that we want to train.\n",
    "        random_key: jax.random.PRNGKey()\n",
    "            Used to trigger the initialization functions, which generate\n",
    "            the initial set of parameters that the model will use.\n",
    "        shape: tuple\n",
    "            Shape of the batch of data that will be input into the model.\n",
    "            This is used to trigger shape inference, which is where the model\n",
    "            figures out by itself what the correct size the weights should be\n",
    "            when they see the inputs.\n",
    "        learning_rate: int\n",
    "            How large of a step the optimizer should take.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        train_state.TrainState:\n",
    "            A utility class for handling parameter and gradient updates. \n",
    "    \"\"\"\n",
    "    # Initialize the model\n",
    "    variables = model.init(random_key, jnp.ones(shape))\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = optax.adam(learning_rate) # TODO update this to be user defined\n",
    "\n",
    "    # Create a state\n",
    "    return train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                         tx=optimizer,\n",
    "                                         params=variables['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cef079-5d40-4e80-b707-171046eaea3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Create function to compute the metrics for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43c963b6-02e9-4e56-8249-c90071b30d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, pred, labels):\n",
    "    \"\"\"\n",
    "    Function that computes metrics that will be logged\n",
    "    during training\n",
    "    \"\"\"\n",
    "    # Calculate the MSE loss\n",
    "    loss = ((pred - labels) ** 2).mean()\n",
    "\n",
    "    # Calculate the R^2 score\n",
    "    residual = jnp.sum(jnp.square(labels - pred))\n",
    "    total = jnp.sum(jnp.square(labels - jnp.mean(labels)))\n",
    "    r2_score = 1 - (residual / total)\n",
    "\n",
    "    # Save these metrics into a dict\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'r2': r2_score\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def accumulate_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Function that accumulates all the metrics for each batch and \n",
    "    accumulates/calculates the metrics for each epoch.\n",
    "    \"\"\"\n",
    "    metrics = jax.device_get(metrics)\n",
    "    return {\n",
    "        k: np.mean([metric[k] for metric in metrics])\n",
    "        for k in metrics[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5970d3e-3199-4005-9e35-f6aaf2953566",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create a training step that does a gradient update for a single batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350ac054-94d0-4794-85c6-cae7affd2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state: train_state.TrainState,\n",
    "               batch: list):\n",
    "    \"\"\"\n",
    "    Function to run training on one batch of data.\n",
    "    \"\"\"\n",
    "    image, label = batch\n",
    "\n",
    "    def loss_fn(params: dict):\n",
    "        \"\"\"\n",
    "        Simple MSE loss as described in the PFGM paper.\n",
    "        \"\"\"\n",
    "        pred = state.apply_fn({'params': params}, image)\n",
    "        loss = ((pred - label) ** 2).mean()\n",
    "        return loss, pred\n",
    "\n",
    "    def r_squared(params):\n",
    "        \"\"\"\n",
    "        Function to calculate the coefficient of determination or \n",
    "        R^2, which quantifies how well the regression model fits \n",
    "        the observed data. Or more formally, it is a statistical\n",
    "        measure that represents the proportion of variance in the\n",
    "        dependent variable that is explained by the independent \n",
    "        variable(s) in a regression model. R^2 ranges from 0 to 1, \n",
    "        with a higher value indicating a better fit. \n",
    "\n",
    "        An R^2 of 0 means that the regression model does not explain\n",
    "        any of the variability in the dependent variable, while an\n",
    "        R^2 of 1 indicates that the regression model explains all of\n",
    "        the variability in the dependent model.\n",
    "        \"\"\"\n",
    "        pred = state.apply_fn({'params': params}, image)\n",
    "        residual = jnp.sum(jnp.square(label - pred))\n",
    "        total = jnp.sum(jnp.square(label - jnp.mean(label)))\n",
    "        r2_score = 1 - (residual / total)\n",
    "        return r2_score\n",
    "\n",
    "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (_, pred), grads = gradient_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(pred=pred, labels=label)\n",
    "    return state, metrics\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state, batch):\n",
    "    image, label = batch\n",
    "    pred = state.apply_fn({'params': state.params}, image)\n",
    "    return compute_metrics(pred=pred, labels=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c6db5-3ac0-4c37-b7cc-34e65ff43112",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Create functions to save and restore the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927952b0-7d98-4f31-a72e-f1bd29ca2539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_wandb(ckpt_path, state, epoch):\n",
    "    with open(ckpt_path, \"wb\") as outfile:\n",
    "        outfile.write(msgpack_serialize(to_state_dict(state)))\n",
    "    artifact = wandb.Artifact(\n",
    "        f'{wandb.run.name}-checkpoint', type='dataset'\n",
    "    )\n",
    "    artifact.add_file(ckpt_path)\n",
    "    wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "    \n",
    "def load_checkpoint_wandb(ckpt_file, state):\n",
    "    artifact = wandb.use_artifact(\n",
    "        f'{wandb.run.name}-checkpoint:latest'\n",
    "    )\n",
    "    artifact_dir = artifact.download()\n",
    "    ckpt_path = os.path.join(artifact_dir, ckpt_file)\n",
    "    with open(ckpt_path, \"rb\") as data_file:\n",
    "        byte_data = data_file.read()\n",
    "    return from_bytes(state, byte_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632b744-f0d2-45fb-8cee-68ea9e92f25c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.1 Necessary things to get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73b1ea44-cc8f-4f00-a7a2-d73a38d15c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "class PerturbMNIST(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset class that stores the data and targets as NumPy arrays.\n",
    "    \n",
    "    Args:\n",
    "    -----\n",
    "        data: np.ndarray\n",
    "            The perturbed input data.\n",
    "        targets: np.ndarray\n",
    "            The empirical field that generated the perturbed data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Returns the i-th sample and corresponding target in the dataset.\n",
    "        \n",
    "        Args:\n",
    "        -----\n",
    "            idx: int\n",
    "                The index of the sample to return.\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "            tuple: A tuple containing the sample and target.\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        return sample, target\n",
    "    \n",
    "    \n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def create_data_loaders(*datasets : Sequence[data.Dataset],\n",
    "                        train : Union[bool, Sequence[bool]] = True,\n",
    "                        batch_size : int = 128,\n",
    "                        num_workers : int = 4,\n",
    "                        seed : int = 42):\n",
    "    \"\"\"\n",
    "    Creates data loaders used in JAX for a set of datasets.\n",
    "\n",
    "    Args:\n",
    "      datasets: Datasets for which data loaders are created.\n",
    "      train: Sequence indicating which datasets are used for\n",
    "        training and which not. If single bool, the same value\n",
    "        is used for all datasets.\n",
    "      batch_size: Batch size to use in the data loaders.\n",
    "      num_workers: Number of workers for each dataset.\n",
    "      seed: Seed to initialize the workers and shuffling with.\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "    if not isinstance(train, (list, tuple)):\n",
    "        train = [train for _ in datasets]\n",
    "    for dataset, is_train in zip(datasets, train):\n",
    "        loader = torch.utils.data.DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=is_train,\n",
    "                                 drop_last=is_train,\n",
    "                                 collate_fn=numpy_collate,\n",
    "                                 num_workers=num_workers,\n",
    "                                 persistent_workers=is_train,\n",
    "                                 generator=torch.Generator().manual_seed(seed))\n",
    "        loaders.append(loader)\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f839ed1-c9fd-4d91-9baa-021d7ad9be07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Create a training loop function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d7d182-e199-480c-8acf-74d1b056c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(batchsize, state, epochs, ckpt_dir, prng):    \n",
    "\n",
    "    # Load the partitioned datasets\n",
    "    training = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                         data_file='partitioned_training_set.pkl')\n",
    "\n",
    "    val = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                         data_file='partitioned_val_set.pkl')\n",
    "\n",
    "    test = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                         data_file='partitioned_test_set.pkl')\n",
    "\n",
    "\n",
    "    # Put them into Pytorch Dataset Objects\n",
    "    training = PerturbMNIST(training[0], training[1])\n",
    "    val = PerturbMNIST(val[0], val[1])\n",
    "    testing = PerturbMNIST(test[0], test[1])\n",
    "    \n",
    "    # Instantiate the pytorch data loaders\n",
    "    train_dl, val_dl, test_dl = create_data_loaders(training, val, testing,\n",
    "                                                    train=[True, False, False],\n",
    "                                                    batch_size=batchsize)\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        best_val_loss = 1e6\n",
    "\n",
    "        # =========== Training =========== #\n",
    "        train_batch_metrics = []\n",
    "        for cnt, batch in enumerate(train_dl):\n",
    "            # Instantiate the perturbed data\n",
    "            data, targets = batch\n",
    "            data = mkds.reshape_with_channel_dim(data)\n",
    "            perturbed_batch = mkds.empirical_field(data, prng)\n",
    "            # Do one train step with perturbed data\n",
    "            state, metrics = train_step(state, perturbed_batch)\n",
    "            train_batch_metrics.append(metrics)\n",
    "        train_batch_metrics = accumulate_metrics(train_batch_metrics)\n",
    "        print(\n",
    "            'TRAIN (%d/%d): Loss: %.4f, r2: %.2f' % (\n",
    "                epoch, epochs, train_batch_metrics['loss'], \n",
    "                train_batch_metrics['r2'])\n",
    "        )\n",
    "                \n",
    "        # =========== Validation =========== #\n",
    "        val_batch_metrics = []\n",
    "        for cnt, batch in enumerate(val_dl):\n",
    "            # Instantiate the imgs\n",
    "            data, targets = batch\n",
    "            data = mkds.reshape_with_channel_dim(data)\n",
    "            # Perturb the data\n",
    "            perturbed_batch = mkds.empirical_field(data, prng)\n",
    "            metrics = eval_step(state, perturbed_batch)\n",
    "            val_batch_metrics.append(metrics)\n",
    "        val_batch_metrics = accumulate_metrics(val_batch_metrics)\n",
    "        print(\n",
    "            'Val (%d/%d): Loss: %.4f, r2: %.2f' % (\n",
    "                epoch, epochs, val_batch_metrics['loss'], \n",
    "                val_batch_metrics['r2'])\n",
    "        )\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Train Loss\": train_batch_metrics['loss'],\n",
    "            \"Train r2\": train_batch_metrics['r2'],\n",
    "            \"Validation Loss\": val_batch_metrics['loss'],\n",
    "            \"Validation r2\": val_batch_metrics['r2']\n",
    "        }, step=epoch)\n",
    "        \n",
    "        if val_batch_metrics['loss'] < best_val_loss:\n",
    "            save_checkpoint_wandb(\"checkpoint.msgpack\", state, epoch)\n",
    "            \n",
    "    restored_state = load_checkpoint_wandb(\"checkpoint.msgpack\", state)\n",
    "    test_batch_metrics = []\n",
    "    for cnt, batch in enumerate(test_dl):\n",
    "            # Instantiate the imgs\n",
    "            data, targets = batch\n",
    "            data = mkds.reshape_with_channel_dim(data)\n",
    "            # Perturb the data\n",
    "            perturbed_batch = mkds.empirical_field(data, prng)\n",
    "            metrics = eval_step(state, perturbed_batch)\n",
    "            test_batch_metrics.append(metrics)\n",
    "        \n",
    "    test_batch_metrics = accumulate_metrics(test_batch_metrics)\n",
    "    print(\n",
    "        'Test: Loss: %.4f, r2: %.2f' % (\n",
    "            test_batch_metrics['loss'],\n",
    "            test_batch_metrics['r2']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    wandb.log({\n",
    "        \"Test Loss\": test_batch_metrics['loss'],\n",
    "        \"Test r2\": test_batch_metrics['r2']\n",
    "    })\n",
    "\n",
    "    # Save best state\n",
    "    checkpoints.save_checkpoint(ckpt_dir, target=restored_state, step=None)\n",
    "    return state, restored_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d599dc-57fb-478f-a0a0-6e691f2a7fc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f4bc1b-1638-4e7c-ab06-10cb99021f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/pscratch/sd/m/mdowicz/PFGM_MNIST/wandb/run-20230327_162604-ads4edts</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mdowicz/MLP_PFGM/runs/ads4edts' target=\"_blank\">hardy-totem-101</a></strong> to <a href='https://wandb.ai/mdowicz/MLP_PFGM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mdowicz/MLP_PFGM' target=\"_blank\">https://wandb.ai/mdowicz/MLP_PFGM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mdowicz/MLP_PFGM/runs/ads4edts' target=\"_blank\">https://wandb.ai/mdowicz/MLP_PFGM/runs/ads4edts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='MLP_PFGM')\n",
    "\n",
    "config = wandb.config\n",
    "config.jax_seed = 42\n",
    "config.batch_size = 256\n",
    "config.learning_rate = 1e-4\n",
    "config.epochs = 12\n",
    "config.hidden_dims = [1570, 3140, 1570]\n",
    "config.output_dim = 785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a5a285c-aa0a-4523-bce5-6711a9a5ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                  MLP Summary                                   \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams               \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ MLP    │ \u001b[2mfloat32\u001b[0m[1,785]  │ \u001b[2mfloat32\u001b[0m[1,785]  │                       │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,785]  │ \u001b[2mfloat32\u001b[0m[1,1570] │ bias: \u001b[2mfloat32\u001b[0m[1570]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[785,1570]     │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m1,234,020 \u001b[0m\u001b[1;2m(4.9 MB)\u001b[0m    │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_1 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,1570] │ \u001b[2mfloat32\u001b[0m[1,3140] │ bias: \u001b[2mfloat32\u001b[0m[3140]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[1570,3140]    │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m4,932,940 \u001b[0m\u001b[1;2m(19.7 MB)\u001b[0m   │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_2 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,3140] │ \u001b[2mfloat32\u001b[0m[1,1570] │ bias: \u001b[2mfloat32\u001b[0m[1570]   │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[3140,1570]    │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m4,931,370 \u001b[0m\u001b[1;2m(19.7 MB)\u001b[0m   │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│ Dense_3 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,1570] │ \u001b[2mfloat32\u001b[0m[1,785]  │ bias: \u001b[2mfloat32\u001b[0m[785]    │\n",
      "│         │        │                 │                 │ kernel:               │\n",
      "│         │        │                 │                 │ \u001b[2mfloat32\u001b[0m[1570,785]     │\n",
      "│         │        │                 │                 │                       │\n",
      "│         │        │                 │                 │ \u001b[1m1,233,235 \u001b[0m\u001b[1;2m(4.9 MB)\u001b[0m    │\n",
      "├─────────┼────────┼─────────────────┼─────────────────┼───────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m               \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m          Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m12,331,565 \u001b[0m\u001b[1;2m(49.3 MB)\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴─────────────────┴─────────────────┴───────────────────────┘\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                     Total Parameters: 12,331,565 \u001b[0m\u001b[1;2m(49.3 MB)\u001b[0m\u001b[1m                     \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with random weights\n",
    "mlp = MLP(hidden_dims=config.hidden_dims,\n",
    "          output_dim=config.output_dim)\n",
    "\n",
    "print(mlp.tabulate(jax.random.PRNGKey(config.jax_seed), jnp.ones((1, 785))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a349ad71-7f52-4d07-adc5-51983b4347e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the initial train_state object\n",
    "state = init_train_state(model=mlp, \n",
    "                         random_key=jax.random.PRNGKey(config.jax_seed), \n",
    "                         shape=(1, 785), \n",
    "                         learning_rate=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe64ae-bf4d-47e9-92f7-2fa618a3ad8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860ec07a7f8d4db7a0c02974ed292f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (1/12): Loss: 13.0952, r2: -12.11\n",
      "Val (1/12): Loss: 1.0020, r2: -0.00\n",
      "TRAIN (2/12): Loss: 1.0008, r2: -0.00\n",
      "Val (2/12): Loss: 1.0001, r2: -0.00\n",
      "TRAIN (3/12): Loss: 0.9996, r2: -0.00\n",
      "Val (3/12): Loss: 0.9995, r2: -0.00\n",
      "TRAIN (4/12): Loss: 0.9991, r2: -0.00\n",
      "Val (4/12): Loss: 0.9992, r2: -0.00\n",
      "TRAIN (5/12): Loss: 0.9988, r2: -0.00\n",
      "Val (5/12): Loss: 0.9990, r2: -0.00\n",
      "TRAIN (6/12): Loss: 0.9987, r2: -0.00\n",
      "Val (6/12): Loss: 0.9990, r2: -0.00\n"
     ]
    }
   ],
   "source": [
    "prng = jax.random.PRNGKey(21)\n",
    "state, best_state = train_and_evaluate(config.batch_size, \n",
    "                                       state, \n",
    "                                       config.epochs, \n",
    "                                       ckpt_dir='saved_models/',\n",
    "                                       prng=default_rng(seed=np.asarray(prng)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b18c71-2df3-4be2-99c0-7c28513496aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_state = checkpoints.restore_checkpoint(ckpt_dir='saved_models/', target=state)\n",
    "\n",
    "assert jax.tree_util.tree_all(jax.tree_map(lambda x, y: (x == y).all(), best_state.params, restored_state.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566d356-4fdf-47e3-aeae-7f19ae2bf988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cab72-c401-492b-a280-5131f2d362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the perturbed datasets\n",
    "partitioned_training = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                     data_file='partitioned_training_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c6d09-a7bd-469e-8815-b682d7588fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = partitioned_training[0], partitioned_training[1]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218c7026-433d-4683-a5ef-b46e9468310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the perturbed datasets\n",
    "perturbed_training = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                     data_file='partitioned_training_set.pkl')\n",
    "\n",
    "perturbed_val = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                     data_file='partitioned_val_set.pkl')\n",
    "\n",
    "perturbed_test = mkds.load_data(data_dir='saved_data/MNIST/raw/partitioned',\n",
    "                                     data_file='partitioned_test_set.pkl')\n",
    "\n",
    "\n",
    "# Put them into Pytorch Dataset Objects\n",
    "training = PerturbMNIST(perturbed_training[0], perturbed_training[1])\n",
    "val = PerturbMNIST(perturbed_val[0], perturbed_val[1])\n",
    "testing = PerturbMNIST(perturbed_test[0], perturbed_test[1])\n",
    "\n",
    "# Instantiate the pytorch data loaders\n",
    "train_dl, val_dl, test_dl = create_data_loaders(training, val, testing,\n",
    "                                                train=[True, False, False],\n",
    "                                                batch_size=128)\n",
    "\n",
    "test_batch = next(iter(test_dl))\n",
    "data_batch = test_batch[0]\n",
    "target_batch = test_batch[1]\n",
    "\n",
    "pred_batch = restored_state.apply_fn({'params': restored_state.params}, data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d6f8-fce1-46c2-87f6-dc50e7b1aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,8))\n",
    "\n",
    "\n",
    "ax[0].quiver(data_batch[:, 2], data_batch[:,-1],\n",
    "             target_batch[:, 2], target_batch[:,-1]);\n",
    "# This quiver plot highlights the selected pixel in the left panel. Allowing the audience to see how \n",
    "# this one pixel is mapped into the N+1 dimension above the hyperplane.\n",
    "ax[0].set_title(f'True Poisson field')\n",
    "ax[0].set_xlabel(f'Perturbed value')\n",
    "ax[0].set_ylabel(f'Z value')\n",
    "\n",
    "ax[1].quiver(data_batch[:, 2], data_batch[:,-1],\n",
    "             pred_batch[:, 2], pred_batch[:,-1]);\n",
    "# This quiver plot highlights the selected pixel in the left panel. Allowing the audience to see how \n",
    "# this one pixel is mapped into the N+1 dimension above the hyperplane.\n",
    "ax[1].set_title(f'Predicted Poisson field')\n",
    "ax[1].set_xlabel(f'Perturbed value')\n",
    "ax[1].set_ylabel(f'Z value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dad3b5-d2f1-4b54-a3e6-5ddb7d23596e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFGM",
   "language": "python",
   "name": "jax_pfgm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
