{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Sequence, Optional, Tuple, Iterator, Dict, Callable, Union\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Changing fonts to be latex typesetting\n",
    "from matplotlib import rcParams\n",
    "rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "rcParams['font.family'] = 'serif'\n",
    "\n",
    "# JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state, checkpoints\n",
    "import optax\n",
    "\n",
    "# For ODESolver\n",
    "from scipy import integrate\n",
    "\n",
    "# PyTorch for Dataloaders\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# Path to import created files\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/src\")\n",
    "\n",
    "# Import created functions\n",
    "# from data import make_dataset as mkds\n",
    "# from visualization import visualize as vis\n",
    "# from models import train_model as tm\n",
    "# from models import sampling as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_transform(img):\n",
    "    # Input: (28, 28) uint8 [0, 255] torch.Tensor, Output: (28, 28, 1) float32 [0, 1] np array\n",
    "    return np.expand_dims(np.array(img, dtype=np.float32), axis=2) / 255.\n",
    "\n",
    "def numpy_collate(batch: Any):\n",
    "    \"\"\"\n",
    "    Function to provide batches numpy arrays instead of torch tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple, list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "def create_data_loaders(*datasets: Sequence[data.Dataset],\n",
    "                        train: Union[bool, Sequence[bool]] = True,\n",
    "                        batch_size: int = 128,\n",
    "                        num_workers: int = 4,\n",
    "                        seed: int = 42):\n",
    "    \"\"\"\n",
    "    Creates data loaders used in JAX for a set of datasets.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        datasets: Sequence[data.Dataset]\n",
    "            Datasets for which data loaders are created\n",
    "        train: Union[bool, Sequence[bool]]\n",
    "            Sequence indicating which datasets are used for training\n",
    "            and which are not. If single bool, the same value is used\n",
    "            for all datasets.\n",
    "        batch_size: int\n",
    "            Batch size to use in the data loaders.\n",
    "        num_workers: int\n",
    "            Number of workers for each dataset\n",
    "        seed: int\n",
    "            Seed to initialize the workers and shuffling with.\n",
    "    \"\"\"\n",
    "    loaders = []\n",
    "    if not isinstance(train, (list, tuple)):\n",
    "        train = [train for _ in datasets]\n",
    "    for dataset, is_train in zip(datasets, train):\n",
    "        loader = data.DataLoader(dataset,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=is_train,\n",
    "                                 drop_last=is_train,\n",
    "                                 collate_fn=numpy_collate,\n",
    "                                 num_workers=num_workers,\n",
    "                                 persistent_workers=is_train,\n",
    "                                 generator=torch.Generator().manual_seed(seed))\n",
    "        loaders.append(loader)\n",
    "    return loaders\n",
    "\n",
    "\n",
    "def load_data_loaders(batch_size: int = 128,\n",
    "                      ds_path: str = 'saved_data/', \n",
    "                      val_on: bool = True):\n",
    "    \"\"\"\n",
    "    Function to load the created dataloaders.\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        ds_path: str\n",
    "            Path where the datasets should be saved\n",
    "        val_on: bool\n",
    "            Toggle to decide if we want a validation set or just train/test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Converting a uint8 [0, 255] torch.Tensor to float32 [0,1] np.array\n",
    "    test_transform = custom_transform\n",
    "    # For training, we add some augmentation to reduce overfitting.\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                          custom_transform])\n",
    "\n",
    "    if val_on:\n",
    "        # Loading the training dataset. Because val_on = True we need to split it into\n",
    "        # training and validation sets. We also need to do a little trick because the\n",
    "        # validation set should not use the augmentation (ie. having same behavior as\n",
    "        # the test set).\n",
    "        train_dataset = MNIST(root=ds_path + \"train\", \n",
    "                              train=True,\n",
    "                              transform=train_transform,\n",
    "                              download=True)\n",
    "        val_dataset = MNIST(root=ds_path + \"val\",\n",
    "                            train=True,\n",
    "                            transform=test_transform,\n",
    "                            download=True)\n",
    "        # Randomly splitting (with the same seed) the training/validation training sets and then only saving the\n",
    "        # respective datasets for each one. I.e. the training set gets 50,000 samples, while the val set gets 10,000.\n",
    "        train_set, _ = data.random_split(train_dataset, [50000, 10000], generator=torch.Generator().manual_seed(42))\n",
    "        _ , val_set = data.random_split(val_dataset, [50000, 10000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        # Loading the test set\n",
    "        test_set = MNIST(root=ds_path + \"test\",\n",
    "                         train=False,\n",
    "                         transform=test_transform,\n",
    "                         download=True)\n",
    "\n",
    "        # Create the train/val/test data loaders\n",
    "        train_loader, val_loader, test_loader = create_data_loaders(train_set, val_set, test_set,\n",
    "                                                                    train=[True, True, False],\n",
    "                                                                    batch_size=batch_size)\n",
    "\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    else:\n",
    "        # Create train and test sets\n",
    "        train_dataset = MNIST(root=ds_path + \"train\", \n",
    "                              train=True,\n",
    "                              transform=train_transform,\n",
    "                              download=True)\n",
    "        test_set = MNIST(root=ds_path + \"test\",\n",
    "                         train=False,\n",
    "                         transform=test_transform,\n",
    "                         download=True)\n",
    "\n",
    "        # Create train/test dataloaders\n",
    "        train_loader, test_loader = create_data_loaders(train_set, test_set,\n",
    "                                                        train=[True, False],\n",
    "                                                        batch_size=batch_size)\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl = load_data_loaders(batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('PFGM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e151eef46bbff781a0688feac4f2b8ea3cd639d70f4001d0c6cef598b400d9f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
